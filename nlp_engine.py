import json
import re
from typing import Tuple, Optional, Dict, List, Any
import difflib
from enum import Enum

class TextPreprocessor:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    @staticmethod
    def normalize_text(text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        text = text.lower().strip()
        text = re.sub(r'[^\w\s]', ' ', text)  # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        text = re.sub(r'\s+', ' ', text)      # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        return text
    
    @staticmethod
    def extract_keywords(text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        stop_words = {
            '–∫–∞–∫', '–≥–¥–µ', '—á—Ç–æ', '–∫—Ç–æ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º',
            '–º–Ω–µ', '–º–Ω–æ–π', '–º–µ–Ω—è', '—Ç–µ–±–µ', '—Ç–æ–±–æ–π', '—Ç–µ–±—è',
            '—Å–≤–æ–π', '—Å–≤–æ—è', '—Å–≤–æ—ë', '—Å–≤–æ–∏',
            '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏', '—ç—Ç–æ—Ç',
            '–≤–æ—Ç', '—Ç—É—Ç', '—Ç–∞–º', '–∑–¥–µ—Å—å', '—Ç—É–¥–∞',
            '–æ—á–µ–Ω—å', '–ø—Ä–æ—Å—Ç–æ', '–≤–æ–æ–±—â–µ', '—Å–æ–≤—Å–µ–º',
            '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–Ω–∞–¥–æ', '—Ö–æ—á—É', '—Ö–æ—Ç–µ–ª'
        }
        
        words = text.split()
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        return keywords
    
    @staticmethod
    def get_word_variations(word: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π —Å–ª–æ–≤–∞ –¥–ª—è —É—á–µ—Ç–∞ –æ–ø–µ—á–∞—Ç–æ–∫"""
        variations = [word]
        
        # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏ –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        common_typos = {
            '–∞': ['–æ'], '–æ': ['–∞'], '–µ': ['—ç'], '–∏': ['–π', '—ã'],
            '—Ç': ['—Ç—Ç', '–¥'], '–ø': ['–ø–ø', '–±'], '–∫': ['–∫–∫', '–≥'],
            '—Å': ['—Å—Å', '–∑'], '–≤': ['–≤–≤', '—Ñ']
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å –∑–∞–º–µ–Ω–æ–π –ø–æ—Ö–æ–∂–∏—Ö –±—É–∫–≤
        for i, char in enumerate(word):
            if char in common_typos:
                for replacement in common_typos[char]:
                    variation = word[:i] + replacement + word[i+1:]
                    variations.append(variation)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏/–ª–∏—à–Ω–∏–º–∏ –±—É–∫–≤–∞–º–∏ (–¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤)
        if len(word) > 3:
            # –ü—Ä–æ–ø—É—Å–∫ –æ–¥–Ω–æ–π –±—É–∫–≤—ã
            for i in range(len(word)):
                variations.append(word[:i] + word[i+1:])
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∏—à–Ω–µ–π –±—É–∫–≤—ã (–ø–æ–≤—Ç–æ—Ä)
            for i in range(len(word)-1):
                if word[i] == word[i+1]:
                    variations.append(word[:i] + word[i+1:])
        
        return list(set(variations))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏

class FuzzySearcher:
    """–ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    @staticmethod
    def fuzzy_ratio(text1: str, text2: str) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
        # –ë–∞–∑–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        base_ratio = difflib.SequenceMatcher(None, text1, text2).ratio()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        words1 = text1.split()
        words2 = text2.split()
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞–º
        word_overlap = len(set(words1) & set(words2)) / max(len(set(words1)), 1)
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –±—É–∫–≤
        first_letter_score = 0
        if words1 and words2:
            if words1[0][0] == words2[0][0]:
                first_letter_score = 0.2
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
        fuzzy_score = (base_ratio * 0.6) + (word_overlap * 0.3) + (first_letter_score * 0.1)
        
        return fuzzy_score
    
    @staticmethod
    def find_best_fuzzy_match(query: str, candidates: List[str], threshold: float = 0.5) -> Tuple[Optional[str], float]:
        """–ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ –Ω–µ—á–µ—Ç–∫–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        if not candidates:
            return None, 0.0
        
        best_match = None
        best_score = 0.0
        
        for candidate in candidates:
            score = FuzzySearcher.fuzzy_ratio(query, candidate)
            if score > best_score:
                best_score = score
                best_match = candidate
        
        if best_score >= threshold:
            return best_match, best_score
        
        return None, 0.0
    
    @staticmethod
    def soundex_rus(word: str) -> str:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π Soundex –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
        if not word:
            return ""
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–π –±—É–∫–≤—ã
        first_char = word[0].upper()
        
        # –ö–æ–¥—ã –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –±—É–∫–≤
        codes = {
            '–∞': '0', '–±': '1', '–≤': '2', '–≥': '3', '–¥': '4', '–µ': '0', '—ë': '0',
            '–∂': '1', '–∑': '2', '–∏': '0', '–π': '0', '–∫': '3', '–ª': '4', '–º': '5',
            '–Ω': '6', '–æ': '0', '–ø': '1', '—Ä': '2', '—Å': '3', '—Ç': '4', '—É': '0',
            '—Ñ': '1', '—Ö': '2', '—Ü': '3', '—á': '4', '—à': '5', '—â': '6', '—ä': '0',
            '—ã': '0', '—å': '0', '—ç': '0', '—é': '0', '—è': '0'
        }
        
        # –ö–æ–¥–∏—Ä—É–µ–º —Å–ª–æ–≤–æ
        encoded = first_char
        
        for char in word[1:].lower():
            code = codes.get(char, '0')
            if code != '0' and (not encoded or encoded[-1] != code):
                encoded += code
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 4 —Å–∏–º–≤–æ–ª–æ–≤
        encoded = (encoded + '000')[:4]
        
        return encoded
    
    @staticmethod
    def soundex_match(query: str, target: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ Soundex"""
        query_soundex = FuzzySearcher.soundex_rus(query)
        target_soundex = FuzzySearcher.soundex_rus(target)
        
        return query_soundex == target_soundex

class KnowledgeBaseSearcher:
    """–ü–æ–∏—Å–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    def __init__(self, file_path: str = "knowledge_base.json"):
        self.file_path = file_path
        self.kb_data = self._load_knowledge_base()
        self.preprocessor = TextPreprocessor()
        self.fuzzy_searcher = FuzzySearcher()
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.question_index = self._build_index()
    
    def _load_knowledge_base(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
                return data
        except (FileNotFoundError, json.JSONDecodeError) as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            return []
    
    def _build_index(self) -> Dict[str, List[Dict]]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        index = {}
        
        for item in self.kb_data:
            question = item.get('question', '')
            normalized = self.preprocessor.normalize_text(question)
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            keywords = self.preprocessor.extract_keywords(normalized)
            for keyword in keywords:
                if keyword not in index:
                    index[keyword] = []
                index[keyword].append(item)
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ Soundex
            soundex = self.fuzzy_searcher.soundex_rus(question)
            if soundex not in index:
                index[soundex] = []
            index[soundex].append(item)
        
        return index
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
        return self.fuzzy_searcher.fuzzy_ratio(text1, text2)
    
    def find_best_match(
        self, 
        user_question: str, 
        source_type: Optional[str] = None,
        threshold: float = 0.4  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —É—á–µ—Ç–∞ –æ–ø–µ—á–∞—Ç–æ–∫
    ) -> Tuple[Optional[Dict], float]:
        """
        –ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫
        """
        if not self.kb_data:
            return None, 0.0
        
        normalized_question = self.preprocessor.normalize_text(user_question)
        keywords = self.preprocessor.extract_keywords(normalized_question)
        
        best_item = None
        best_confidence = 0.0
        
        # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å (–±—ã—Å—Ç—Ä—ã–π)
        candidate_items = set()
        
        for keyword in keywords:
            if keyword in self.question_index:
                candidate_items.update(self.question_index[keyword])
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å, –∏—â–µ–º –≤–æ –≤—Å–µ–π –±–∞–∑–µ
        if not candidate_items:
            candidate_items = self.kb_data
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —É—á–µ—Ç–∞ –æ–ø–µ—á–∞—Ç–æ–∫
        query_variations = []
        for keyword in keywords[:3]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
            variations = self.preprocessor.get_word_variations(keyword)
            query_variations.extend(variations)
        
        for item in candidate_items:
            item_question = item.get('question', '')
            item_source = item.get('source', 'manual')
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if source_type and item_source != source_type:
                continue
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–æ–ø—Ä–æ—Å –∏–∑ –±–∞–∑—ã
            normalized_item = self.preprocessor.normalize_text(item_question)
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —á–µ—Ä–µ–∑ –Ω–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫
            similarity = self._calculate_similarity(normalized_question, normalized_item)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ Soundex
            soundex_match = self.fuzzy_searcher.soundex_match(
                normalized_question[:10],  # –ë–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                normalized_item[:10]
            )
            
            if soundex_match:
                similarity = max(similarity, 0.6)  # –ü–æ–≤—ã—à–∞–µ–º score –ø—Ä–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ –ø–æ Soundex
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
            for variation in query_variations[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≤–µ—Ä–æ–∫
                if variation in normalized_item:
                    similarity = max(similarity, 0.55)  # –ù–µ–±–æ–ª—å—à–æ–π –±–æ–Ω—É—Å
                    break
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            item_keywords = self.preprocessor.extract_keywords(normalized_item)
            common_keywords = set(keywords) & set(item_keywords)
            keyword_overlap = len(common_keywords) / max(len(keywords), 1)
            
            # –ò—Ç–æ–≥–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
            confidence = (similarity * 0.6) + (keyword_overlap * 0.4)
            
            if confidence > best_confidence:
                best_confidence = confidence
                best_item = item
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–Ω–∏–∂–µ –¥–ª—è —É—á–µ—Ç–∞ –æ–ø–µ—á–∞—Ç–æ–∫)
        if best_confidence >= threshold:
            return best_item, best_confidence
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ø–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç—è–º –≤–æ–ø—Ä–æ—Å–∞
        if len(keywords) > 1:
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            for item in self.kb_data:
                if source_type and item.get('source') != source_type:
                    continue
                    
                item_text = self.preprocessor.normalize_text(item.get('question', ''))
                matches = sum(1 for kw in keywords if kw in item_text)
                
                if matches >= 2:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 2 —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    confidence = matches / len(keywords)
                    if confidence > best_confidence:
                        best_confidence = confidence
                        best_item = item
        
        if best_confidence >= threshold * 0.8:  # –ï—â–µ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥
            return best_item, best_confidence
        
        return None, 0.0
    
    def find_by_exact_question(
        self, 
        question: str, 
        source_type: Optional[str] = None
    ) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –≤–æ–ø—Ä–æ—Å—É"""
        normalized_question = self.preprocessor.normalize_text(question)
        
        for item in self.kb_data:
            item_question = self.preprocessor.normalize_text(item.get('question', ''))
            item_source = item.get('source', 'manual')
            
            if source_type and item_source != source_type:
                continue
            
            if item_question == normalized_question:
                return item
        
        return None

class IntentClassifier:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    
    def __init__(self):
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞–º–µ—Ä–µ–Ω–∏–π —Å –≤–µ—Å–∞–º–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        self.intent_patterns = {
            'payment_to_supplier': {
                'primary': ['–æ–ø–ª–∞—Ç[–∏—å]—Ç?—å?', '–ø–ª–∞—Ç–µ–∂', '–ø–µ—Ä–µ—á–∏—Å–ª[–µ–∏]—Ç?—å?', '—Å–ø–∏—Å–∞–Ω[–∏–µ]', '–≤—ã–ø–ª–∞—Ç[–∏]—Ç?—å?'],
                'secondary': ['–ø–æ—Å—Ç–∞–≤—â–∏–∫[—É–∞]?', '–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç—É', '–ø–æ—Å—Ç–∞–≤–∫—É', '–∑–∞ —Ç–æ–≤–∞—Ä[—ã]?', '–∑–∞ —É—Å–ª—É–≥—É'],
                'negative': ['–æ–ø—Ä–∏—Ö–æ–¥', '–ø–æ—Å—Ç—É–ø–ª', '–ø–æ–ª—É—á[–µ–∏]', '–ø—Ä–∏–Ω—è—Ç', '—Ç–æ–≤–∞—Ä[—ã]? –æ—Ç'],
                'weight': 2.0,
                'threshold': 1.5
            },
            'goods_receipt': {
                'primary': ['–æ–ø—Ä–∏—Ö–æ–¥[–∞–æ]–≤[–∞]—Ç?—å?', '–ø–æ—Å—Ç—É–ø–ª[–µ–∏]', '–ø–æ–ª—É—á[–µ–∏]–ª?', '–ø—Ä–∏–Ω—è—Ç[—å]', '–ø—Ä–∏–µ–º–∫[–∞—É]'],
                'secondary': ['—Ç–æ–≤–∞—Ä[—ã]?', '–º–∞—Ç–µ—Ä–∏–∞–ª[—ã]?', '—Ç–º—Ü', '–æ—Ç –ø–æ—Å—Ç–∞–≤—â–∏–∫', '–∫—É–ø–∏–ª', '–∑–∞–∫—É–ø–∏–ª'],
                'negative': ['–æ–ø–ª–∞—Ç', '–ø–ª–∞—Ç–µ–∂', '–ø–µ—Ä–µ—á–∏—Å–ª', '–¥–µ–Ω—å–≥', '—Å–ø–∏—Å–∞–Ω'],
                'weight': 2.0,
                'threshold': 1.5
            },
            'invoice_creation': {
                'primary': ['–Ω–∞–∫–ª–∞–¥–Ω[–∞—É—é]', '—Å—á–µ—Ç[- ]?—Ñ–∞–∫—Ç—É—Ä[—É–∞]?', '—É–ø–¥', '—Ç–æ—Ä–≥[- ]?12'],
                'secondary': ['—Å–æ–∑–¥–∞[—Ç—å]', '–≤—ã–ø–∏—Å–∞—Ç[—å]', '–æ—Ñ–æ—Ä–º–∏—Ç[—å]', '–ø—Ä–æ–≤–µ—Å—Ç–∏'],
                'negative': ['–æ–ø–ª–∞—Ç', '–ø–æ–ª—É—á[–µ–∏]–ª?', '–ø—Ä–∏–Ω—è–ª'],
                'weight': 1.8,
                'threshold': 1.3
            },
            'bank_statement': {
                'primary': ['–≤—ã–ø–∏—Å–∫[–∞—É]', '–±–∞–Ω–∫–æ–≤—Å–∫[–∞—É—é]', '–∑–∞–≥—Ä—É–∑[–∏]—Ç?—å?', '–∏–º–ø–æ—Ä—Ç[–∏]'],
                'secondary': ['–±–∞–Ω–∫[–∞–µ—É]?', '—Å—á–µ—Ç[–∞—É–µ]?', '–æ–ø–µ—Ä–∞—Ü–∏[–∏–π]', '–ø–ª–∞—Ç–µ–∂[–µ–∏]'],
                'negative': ['–∫–∞—Å—Å', '–Ω–∞–ª–∏—á–Ω'],
                'weight': 1.5,
                'threshold': 1.2
            },
            'cash_operations': {
                'primary': ['–∫–∞—Å—Å[–∞—É–µ—ã]', '–Ω–∞–ª–∏—á–Ω[—ã–µ]', '–ø–∫–æ', '—Ä–∫–æ', '–æ—Ä–¥–µ—Ä[–∞—É]'],
                'secondary': ['–ø—Ä–∏—Ö–æ–¥–Ω[–æ–π]', '—Ä–∞—Å—Ö–æ–¥–Ω[–æ–π]', '–≤—ã–¥–∞—Ç[—å]', '–ø–æ–ª—É—á[–∏]—Ç?—å?'],
                'negative': ['–±–µ–∑–Ω–∞–ª', '–±–∞–Ω–∫', '–ø–µ—Ä–µ—á–∏—Å–ª'],
                'weight': 1.5,
                'threshold': 1.2
            },
            'report_generation': {
                'primary': ['–æ—Ç—á–µ—Ç[–∞—É]?', '–≤–µ–¥–æ–º–æ—Å—Ç[—å–∏]', '–∞–Ω–∞–ª–∏–∑[–∞—É]?', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫[–∞—É]'],
                'secondary': ['—Å—Ñ–æ—Ä–º–∏—Ä[–æ–∞]–≤[–∞]—Ç?—å?', '–ø–æ—Å–º–æ—Ç—Ä[–µ]—Ç?—å?', '–ø–æ–ª—É—á–∏—Ç[—å]', '–ø–æ—Å—Ç—Ä–æ–∏—Ç[—å]'],
                'negative': ['–¥–æ–∫—É–º–µ–Ω—Ç', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '—Å–æ–∑–¥–∞—Ç[—å]'],
                'weight': 1.3,
                'threshold': 1.0
            },
            'debt_analysis': {
                'primary': ['–∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç[—å–∏]', '–¥–µ–±–∏—Ç–æ—Ä—Å–∫[–∞—É—é]', '–∫—Ä–µ–¥–∏—Ç–æ—Ä—Å–∫[–∞—É—é]', '–¥–æ–ª–≥[–∏–∞]'],
                'secondary': ['–ø–æ—Å–º–æ—Ç—Ä[–µ]—Ç?—å?', '–ø—Ä–æ–≤–µ—Ä–∏—Ç[—å]', '–ø—Ä–æ–∞–Ω–∞–ª–∏–∑[–∏]', '–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç'],
                'negative': ['–æ–ø–ª–∞—Ç', '–ø–µ—Ä–µ—á–∏—Å–ª', '–ø—Ä–æ–≤–µ—Å—Ç–∏'],
                'weight': 1.4,
                'threshold': 1.1
            },
            'advance_report': {
                'primary': ['–∞–≤–∞–Ω—Å–æ–≤[—ã–π]', '–ø–æ–¥–æ—Ç—á–µ—Ç[–Ω]', '–æ—Ç—á–µ—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏–∫'],
                'secondary': ['—Å–æ–∑–¥–∞[—Ç—å]', '–∑–∞–ø–æ–ª–Ω–∏[—Ç—å]', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '—Å–¥–∞[—Ç—å]'],
                'negative': ['–≤—ã–¥–∞—Ç[—å]', '–ø–æ–ª—É—á[–∏]—Ç?—å?', '–¥–µ–Ω—å–≥[–∏] –ø–æ–¥'],
                'weight': 1.4,
                'threshold': 1.1
            },
            'goods_balance': {
                'primary': ['–æ—Å—Ç–∞—Ç–∫[–∏–∞—É]', '–Ω–∞–ª–∏—á[–∏–µ]', '—Å–∫–ª–∞–¥[–∞–µ—É]', '–∑–∞–ø–∞—Å[—ã–∞]'],
                'secondary': ['—Ç–æ–≤–∞—Ä[—ã]?', '–ø–æ—Å–º–æ—Ç—Ä[–µ]—Ç?—å?', '–ø—Ä–æ–≤–µ—Ä–∏—Ç[—å]', '—Å–∫–æ–ª—å–∫–æ –µ—Å—Ç—å'],
                'negative': ['–ø—Ä–æ–¥–∞–∂', '–æ—Ç–≥—Ä—É–∑–∫', '—Ä–µ–∞–ª–∏–∑–∞—Ü'],
                'weight': 1.3,
                'threshold': 1.0
            },
            'sales_period': {
                'primary': ['–ø—Ä–æ–¥–∞–∂[–∏] –ø–æ', '–¥–∏–Ω–∞–º–∏–∫[–∞—É]', '–ø–µ—Ä–∏–æ–¥[–∞—É—ã]?', '–º–µ—Å—è—Ü[–∞—É—ã]?'],
                'secondary': ['–≥—Ä–∞—Ñ–∏–∫', '—Ç–µ–Ω–¥–µ–Ω—Ü', '—Å—Ä–∞–≤–Ω–µ–Ω[–∏–µ]'],
                'negative': ['–æ–ø—Ä–∏—Ö–æ–¥', '–ø–æ—Å—Ç—É–ø–ª', '–∑–∞–∫—É–ø–∫'],
                'weight': 1.3,
                'threshold': 1.0
            },
            'greeting': {
                'primary': ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä[—ã–π–æ–µ]', 'hello', 'hi', '–∑–¥—Ä–∞—Å—Ç–µ'],
                'secondary': [],
                'negative': [],
                'weight': 3.0,
                'threshold': 0.5
            },
            'farewell': {
                'primary': ['–ø–æ–∫–∞', '–¥–æ —Å–≤–∏–¥–∞–Ω[–∏—è]', '–≤—ã—Ö–æ–¥', '–∑–∞–∫–æ–Ω—á[–∏]—Ç?—å?', '—Å–ø–∞—Å–∏–±–æ'],
                'secondary': [],
                'negative': [],
                'weight': 3.0,
                'threshold': 0.5
            },
            'help_request': {
                'primary': ['–ø–æ–º–æ—â[—å–∏]', '–ø–æ–º–æ–≥[–∏]', '–ø–æ–¥—Å–∫–∞–∂[–∏]', '–ø–æ—Å–æ–≤–µ—Ç—É[–π–∏]'],
                'secondary': ['—á—Ç–æ —Ç—ã —É–º–µ–µ—à[—å]', '–∫–æ–º–∞–Ω–¥[—ã]', '–∏–Ω—Å—Ç—Ä—É–∫—Ü[–∏—è]'],
                'negative': [],
                'weight': 2.5,
                'threshold': 0.8
            },
            'button_click': {
                'primary': ['button:', 'menu:', '–∫–Ω–æ–ø–∫[–∞—É–∏]', '–∫–ª–∏–∫[–∞—É]'],
                'secondary': ['–Ω–∞–∂–∞—Ç[—å]', '–Ω–∞–∂–º[–∏]', '–≤—ã–±—Ä–∞—Ç[—å]', '–Ω–∞–∂—ã', '–∫–ª–∏–∫–Ω[—É—Ç—å]'],
                'negative': [],
                'weight': 2.0,
                'threshold': 0.7
            },
            'unknown': {
                'primary': [],
                'secondary': [],
                'negative': [],
                'weight': 0.0,
                'threshold': 0.0
            }
        }
    
    def classify(self, text: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–µ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()
        detected_intents = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        for intent_type in ['greeting', 'farewell', 'help_request', 'button_click']:
            if intent_type == 'button_click':
                if 'button:' in text_lower or 'menu:' in text_lower:
                    detected_intents.append('button_click')
            else:
                for pattern in self.intent_patterns[intent_type]['primary']:
                    if re.search(pattern, text_lower):
                        detected_intents.append(intent_type)
                        break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –±–∞–∑–æ–≤—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        if not detected_intents:
            main_intent = self._classify_with_context(text_lower)
            detected_intents.append(main_intent)
        
        return detected_intents if detected_intents else ['unknown']
    
    def _classify_with_context(self, text: str) -> str:
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤"""
        scores = {}
        
        for intent_name, patterns in self.intent_patterns.items():
            if intent_name in ['greeting', 'farewell', 'help_request', 'button_click', 'unknown']:
                continue
                
            score = 0.0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in patterns['primary']:
                if re.search(pattern, text):
                    score += patterns['weight'] * 2.0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ç–æ—Ä–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in patterns['secondary']:
                if re.search(pattern, text):
                    score += patterns['weight'] * 1.0
            
            # –®—Ç—Ä–∞—Ñ—É–µ–º –∑–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
            for pattern in patterns['negative']:
                if re.search(pattern, text):
                    score -= patterns['weight'] * 3.0  # –°–∏–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ
            
            scores[intent_name] = max(score, 0.0)
        
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω—Ç–µ–Ω—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å—á–µ—Ç–æ–º
        if scores:
            best_intent = max(scores, key=scores.get)
            if scores[best_intent] >= self.intent_patterns[best_intent]['threshold']:
                return best_intent
        
        return 'unknown'
    
    def classify_with_context(self, text: str) -> Tuple[str, float]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        text_lower = text.lower()
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—ã–µ –∏–Ω—Ç–µ–Ω—Ç—ã
        if 'button:' in text_lower or 'menu:' in text_lower:
            return 'button_click', 1.0
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        scores = {}
        for intent_name, patterns in self.intent_patterns.items():
            if intent_name in ['greeting', 'farewell', 'help_request', 'button_click', 'unknown']:
                continue
                
            score = 0.0
            primary_matches = 0
            secondary_matches = 0
            negative_matches = 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in patterns['primary']:
                if re.search(pattern, text_lower):
                    score += patterns['weight'] * 2.0
                    primary_matches += 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ç–æ—Ä–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in patterns['secondary']:
                if re.search(pattern, text_lower):
                    score += patterns['weight'] * 1.0
                    secondary_matches += 1
            
            # –®—Ç—Ä–∞—Ñ—É–µ–º –∑–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
            for pattern in patterns['negative']:
                if re.search(pattern, text_lower):
                    score -= patterns['weight'] * 3.0
                    negative_matches += 1
            
            # –£—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            total_matches = primary_matches + secondary_matches
            if total_matches > 0:
                match_ratio = primary_matches / total_matches
                score *= (0.5 + match_ratio * 0.5)
            
            scores[intent_name] = max(score, 0.0)
        
        if not scores:
            return 'unknown', 0.0
        
        best_intent = max(scores, key=scores.get)
        best_score = scores[best_intent]
        threshold = self.intent_patterns[best_intent]['threshold']
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = min(best_score / (threshold * 2), 1.0) if threshold > 0 else 0.5
        
        if best_score >= threshold:
            return best_intent, confidence
        
        return 'unknown', confidence
    
    def is_button_click(self, text: str) -> Tuple[bool, Optional[str], Optional[str]]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–∞–∂–∞—Ç–∏–µ–º –∫–Ω–æ–ø–∫–∏"""
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç—ã: "button:–Ω–∞–∫–ª–∞–¥–Ω—ã–µ" –∏–ª–∏ "menu:–æ—Ç—á–µ—Ç—ã"
        for prefix in ['button:', 'menu:']:
            if text_lower.startswith(prefix):
                parts = text_lower.split(':', 1)
                if len(parts) == 2:
                    return True, prefix.rstrip(':'), parts[1].strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (—Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫)
        button_patterns = [
            (['–Ω–∞–∂–∞—Ç—å –∫–Ω–æ–ø–∫—É', '–Ω–∞–∂–º–∏ –∫–Ω–æ–ø–∫—É', '–Ω–∞–∂—ã –∫–Ω–æ–ø–∫—É', '–Ω–∞–∂–∞—Ç—å–∫–Ω–æ–ø–∫—É'], 'button'),
            (['–∫–ª–∏–∫ –ø–æ –∫–Ω–æ–ø–∫–µ', '–∫–ª–∏–∫–Ω—É—Ç—å –∫–Ω–æ–ø–∫—É', '–∫–ª–∏–∫ –ø–æ', '–∫–ª–∏–∫–Ω—É—Ç—å'], 'button'),
            (['–≤ –º–µ–Ω—é', '–º–µ–Ω—é', '–≤ —Ä–∞–∑–µ–¥–µ–ª', '—Ä–∞–∑–µ–¥–µ–ª'], 'menu'),
            (['—Ä–∞–∑–¥–µ–ª', '—Ä–∞–∑–¥–∏–ª', '—Ä–∞–¥–µ–ª'], 'menu')
        ]
        
        for patterns, source_type in button_patterns:
            for pattern in patterns:
                if pattern in text_lower:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
                    start_idx = text_lower.find(pattern) + len(pattern)
                    button_text = text_lower[start_idx:].strip()
                    if button_text:
                        return True, source_type, button_text
        
        return False, None, None
    
    def get_intent_description(self, intent_name: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–Ω—Ç–µ–Ω—Ç–∞"""
        descriptions = {
            'payment_to_supplier': '–û–ø–ª–∞—Ç–∞ –ø–æ—Å—Ç–∞–≤—â–∏–∫—É',
            'goods_receipt': '–û–ø—Ä–∏—Ö–æ–¥–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ –æ—Ç –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞',
            'invoice_creation': '–°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞–∫–ª–∞–¥–Ω–æ–π –∏–ª–∏ —Å—á–µ—Ç–∞-—Ñ–∞–∫—Ç—É—Ä—ã',
            'bank_statement': '–†–∞–±–æ—Ç–∞ —Å –±–∞–Ω–∫–æ–≤—Å–∫–∏–º–∏ –≤—ã–ø–∏—Å–∫–∞–º–∏',
            'cash_operations': '–ö–∞—Å—Å–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏',
            'report_generation': '–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤',
            'debt_analysis': '–ê–Ω–∞–ª–∏–∑ –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç–∏',
            'advance_report': '–ê–≤–∞–Ω—Å–æ–≤—ã–µ –æ—Ç—á–µ—Ç—ã',
            'goods_balance': '–û—Å—Ç–∞—Ç–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤',
            'sales_period': '–ü—Ä–æ–¥–∞–∂–∏ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º',
            'greeting': '–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ',
            'farewell': '–ü—Ä–æ—â–∞–Ω–∏–µ',
            'help_request': '–ó–∞–ø—Ä–æ—Å –ø–æ–º–æ—â–∏',
            'button_click': '–ù–∞–∂–∞—Ç–∏–µ –∫–Ω–æ–ø–∫–∏',
            'unknown': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å'
        }
        return descriptions.get(intent_name, '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏–Ω—Ç–µ–Ω—Ç')

class ButtonHandler:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏–π –∫–Ω–æ–ø–æ–∫ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    def __init__(self, kb_searcher: KnowledgeBaseSearcher):
        self.kb_searcher = kb_searcher
        self.preprocessor = TextPreprocessor()
    
    def handle_button_click(
        self, 
        source_type: str, 
        button_text: str
    ) -> Optional[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
        print(f"üîò –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–Ω–æ–ø–∫–∏: source={source_type}, text='{button_text}'")
        
        normalized_button = self.preprocessor.normalize_text(button_text)
        
        # 1. –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        exact_match = self.kb_searcher.find_by_exact_question(
            normalized_button, 
            source_type=source_type
        )
        
        if exact_match:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª—è –∫–Ω–æ–ø–∫–∏ '{button_text}'")
            return exact_match
        
        # 2. –ò—â–µ–º —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫ (–Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥)
        fuzzy_match, confidence = self.kb_searcher.find_best_match(
            normalized_button,
            source_type=source_type,
            threshold=0.3  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –∫–Ω–æ–ø–æ–∫
        )
        
        if fuzzy_match and confidence >= 0.3:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –Ω–µ—á–µ—Ç–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            return fuzzy_match
        
        # 3. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º source, –∏—â–µ–º –≤ –ª—é–±–æ–º source
        any_match, confidence = self.kb_searcher.find_best_match(
            normalized_button,
            threshold=0.35
        )
        
        if any_match:
            print(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –¥—Ä—É–≥–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            return any_match
        
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è –∫–Ω–æ–ø–∫–∏ '{button_text}'")
        return None

class NLPEngine:
    """–û—Å–Ω–æ–≤–Ω–æ–π NLP-–¥–≤–∏–∂–æ–∫ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.preprocessor = TextPreprocessor()
        self.intent_classifier = IntentClassifier()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.kb_searcher = KnowledgeBaseSearcher()
        self.button_handler = ButtonHandler(self.kb_searcher)
    
    def process_message(self, user_message: str) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
        """
        print(f"\nüì® –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: '{user_message}'")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –Ω–∞–∂–∞—Ç–∏–µ–º –∫–Ω–æ–ø–∫–∏
        is_button_click, source_type, button_text = self.intent_classifier.is_button_click(
            user_message
        )
        
        if is_button_click and source_type and button_text:
            print(f"üéØ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–∞–∫ –Ω–∞–∂–∞—Ç–∏–µ –∫–Ω–æ–ø–∫–∏: {source_type} -> '{button_text}'")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –∫–Ω–æ–ø–∫—É
            kb_item = self.button_handler.handle_button_click(source_type, button_text)
            
            if kb_item:
                return {
                    'original_message': user_message,
                    'normalized_message': button_text,
                    'intents': ['button_click'],
                    'source_type': source_type,
                    'kb_answer': kb_item.get('answer'),
                    'kb_item': kb_item,
                    'kb_confidence': 1.0,
                    'has_kb_answer': True,
                    'is_button_click': True,
                    'is_fuzzy_match': False
                }
        
        # –û–±—ã—á–Ω–∞—è —Ç–µ–∫—Å—Ç–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        normalized = self.preprocessor.normalize_text(user_message)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        main_intent, intent_confidence = self.intent_classifier.classify_with_context(normalized)
        intent_description = self.intent_classifier.get_intent_description(main_intent)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = self.preprocessor.extract_keywords(normalized)
        
        # –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫
        kb_item, kb_confidence = self.kb_searcher.find_best_match(
            user_message, 
            threshold=0.35
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —ç—Ç–æ fuzzy match
        is_fuzzy_match = False
        if kb_item and kb_confidence < 0.7:
            original_question = kb_item.get('question', '')
            if original_question.lower() != normalized:
                is_fuzzy_match = True
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result = {
            'original_message': user_message,
            'normalized_message': normalized,
            'main_intent': main_intent,
            'intent_description': intent_description,
            'intent_confidence': intent_confidence,
            'keywords': keywords,
            'kb_answer': kb_item.get('answer') if kb_item else None,
            'kb_item': kb_item,
            'kb_confidence': kb_confidence,
            'has_kb_answer': kb_item is not None,
            'is_button_click': False,
            'is_fuzzy_match': is_fuzzy_match
        }
        
        return result
    
    def get_final_answer(self, user_message: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å —É—Ç–æ—á–Ω–µ–Ω–∏—è–º–∏ –ø—Ä–∏ –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        analysis = self.process_message(user_message)
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        if analysis['has_kb_answer']:
            kb_item = analysis['kb_item']
            confidence = analysis['kb_confidence']
            
            # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è (< 65%), –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º —É—Ç–æ—á–Ω–∏—Ç—å
            if confidence < 0.65 and analysis['main_intent'] != 'unknown':
                intent_desc = analysis['intent_description']
                original_q = kb_item.get('question', '')
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ç–µ–Ω—Ç–∞
                clarification_map = {
                    'payment_to_supplier': "—É—Ç–æ—á–Ω–∏—Ç–µ, –≤–∞–º –Ω—É–∂–Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ **–æ–ø–ª–∞—Ç–µ –ø–æ—Å—Ç–∞–≤—â–∏–∫—É** –∏–ª–∏ –ø–æ **–æ–ø—Ä–∏—Ö–æ–¥–æ–≤–∞–Ω–∏—é –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –æ—Ç –Ω–µ–≥–æ —Ç–æ–≤–∞—Ä–∞**?",
                    'goods_receipt': "—É—Ç–æ—á–Ω–∏—Ç–µ, –≤–∞–º –Ω—É–∂–Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ **–æ–ø—Ä–∏—Ö–æ–¥–æ–≤–∞–Ω–∏—é —Ç–æ–≤–∞—Ä–∞ –æ—Ç –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞** –∏–ª–∏ –ø–æ **–æ–ø–ª–∞—Ç–µ –µ–º—É**?",
                    'invoice_creation': "—É—Ç–æ—á–Ω–∏—Ç–µ, –≤–∞–º –Ω—É–∂–Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ **—Å–æ–∑–¥–∞–Ω–∏—é –Ω–∞–∫–ª–∞–¥–Ω–æ–π** –∏–ª–∏ –ø–æ **–µ–µ –æ–ø–ª–∞—Ç–µ/–ø–æ–ª—É—á–µ–Ω–∏—é**?",
                    'debt_analysis': "—É—Ç–æ—á–Ω–∏—Ç–µ, –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç **–¥–µ–±–∏—Ç–æ—Ä—Å–∫–∞—è –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å** (–Ω–∞–º –¥–æ–ª–∂–Ω—ã) –∏–ª–∏ **–∫—Ä–µ–¥–∏—Ç–æ—Ä—Å–∫–∞—è** (–º—ã –¥–æ–ª–∂–Ω—ã)?"
                }
                
                clarification = clarification_map.get(
                    analysis['main_intent'], 
                    "—É—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–∞—à –≤–æ–ø—Ä–æ—Å?"
                )
                
                return f"ü§î **–Ø –Ω–∞—à–µ–ª –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.**\n\n{clarification}\n\n*–ü–æ—Ö–æ–∂–∏–π –≤–æ–ø—Ä–æ—Å –≤ –±–∞–∑–µ: ¬´{original_q}¬ª*"
            
            # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤—ã—Å–æ–∫–∞—è, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç
            answer = kb_item.get('answer', '')
            
            # –î–ª—è –∫–Ω–æ–ø–æ–∫ –¥–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ
            if analysis.get('is_button_click'):
                source = kb_item.get('source', '')
                button_text = kb_item.get('metadata', {}).get('button_text', '')
                
                if button_text and source in ['menu', 'button']:
                    header = f"üîò **{button_text}**\n\n"
                    return header + answer
            
            # –î–ª—è fuzzy match –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ—è—Å–Ω–µ–Ω–∏–µ
            confidence_percent = int(confidence * 100)
            
            if analysis.get('is_fuzzy_match'):
                original_question = kb_item.get('question', '')
                return f"‚úÖ {answer}\n\n<i>(–í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: '{original_question}'. –ù–∞–π–¥–µ–Ω–æ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence_percent}%)</i>"
            else:
                return f"‚úÖ {answer}\n\n<i>(–ù–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence_percent}%)</i>"
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏
        suggestions = self._get_search_suggestions(user_message)
        return f"ü§î <b>–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.</b>\n\n{suggestions}"
    
    def _get_search_suggestions(self, query: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ –ø–æ–∏—Å–∫—É"""
        normalized = self.preprocessor.normalize_text(query)
        keywords = self.preprocessor.extract_keywords(normalized)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ–Ω—Ç –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        main_intent, _ = self.intent_classifier.classify_with_context(normalized)
        intent_desc = self.intent_classifier.get_intent_description(main_intent)
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –≤–æ–ø—Ä–æ—Å—ã –≤ –±–∞–∑–µ
        similar_questions = []
        
        for item in self.kb_searcher.kb_data[:15]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 15
            item_question = self.preprocessor.normalize_text(item.get('question', ''))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            item_keywords = self.preprocessor.extract_keywords(item_question)
            common = set(keywords) & set(item_keywords)
            
            if len(common) >= 1 and item_question not in similar_questions:
                similar_questions.append(item.get('question', ''))
            
            if len(similar_questions) >= 3:
                break
        
        suggestions = "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
        suggestions += "1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–Ω–æ–ø–∫–∏ –º–µ–Ω—é\n"
        suggestions += f"2. –£—Ç–æ—á–Ω–∏—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ —Ç–µ–º–µ: {intent_desc}\n"
        
        if similar_questions:
            suggestions += "3. –í–æ–∑–º–æ–∂–Ω–æ, –≤–∞–º –Ω—É–∂–µ–Ω –æ–¥–∏–Ω –∏–∑ —ç—Ç–∏—Ö —Ä–∞–∑–¥–µ–ª–æ–≤:\n"
            for i, q in enumerate(similar_questions, 1):
                suggestions += f"   ‚Ä¢ {q}\n"
        
        suggestions += "4. –û–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É"
        
        return suggestions

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä NLP-–¥–≤–∏–∂–∫–∞
nlp_engine = NLPEngine()
