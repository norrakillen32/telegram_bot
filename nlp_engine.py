import json
import re
from typing import Tuple, Optional, Dict, List
import difflib

class TextPreprocessor:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    
    @staticmethod
    def normalize_text(text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        text = text.lower().strip()
        text = re.sub(r'[^\w\s]', ' ', text)  # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        text = re.sub(r'\s+', ' ', text)      # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        return text
    
    @staticmethod
    def extract_keywords(text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        stop_words = {
            '–∫–∞–∫', '–≥–¥–µ', '—á—Ç–æ', '–∫—Ç–æ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º',
            '–º–Ω–µ', '–º–Ω–æ–π', '–º–µ–Ω—è', '—Ç–µ–±–µ', '—Ç–æ–±–æ–π', '—Ç–µ–±—è',
            '—Å–≤–æ–π', '—Å–≤–æ—è', '—Å–≤–æ—ë', '—Å–≤–æ–∏',
            '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏', '—ç—Ç–æ—Ç',
            '–≤–æ—Ç', '—Ç—É—Ç', '—Ç–∞–º', '–∑–¥–µ—Å—å', '—Ç—É–¥–∞',
            '–æ—á–µ–Ω—å', '–ø—Ä–æ—Å—Ç–æ', '–≤–æ–æ–±—â–µ', '—Å–æ–≤—Å–µ–º'
        }
        
        words = text.split()
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        return keywords
    
    @staticmethod
    def lemmatize_word(word: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ pymorphy2 –∏–ª–∏ natasha)"""
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        endings = {
            '—Å—è': '', '—Ç—å—Å—è': '', '—Ç—Å—è': '', '—Ç—å—Å—è': '',
            '–∏—è': '–∏', '–∏—é': '–∏', '–∏–∏': '–∏',
            '–∞—è': '–∞', '—É—é': '–∞', '–æ–π': '–∞',
            '—ã–π': '—ã–π', '–æ–≥–æ': '—ã–π', '–æ–º—É': '—ã–π',
            '–∏–µ': '–∏–µ', '–∏—Ö': '–∏–µ', '–∏–º': '–∏–µ'
        }
        
        for ending, replacement in endings.items():
            if word.endswith(ending):
                return word[:-len(ending)] + replacement
        return word

class KnowledgeBaseSearcher:
    """–ü–æ–∏—Å–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    
    def __init__(self, file_path: str = "knowledge_base.json"):
        self.file_path = file_path
        self.kb_data = self._load_knowledge_base()
        self.preprocessor = TextPreprocessor()
    
    def _load_knowledge_base(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return []
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤ (0-1)"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SequenceMatcher –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ç—Ä–æ–∫
        return difflib.SequenceMatcher(None, text1, text2).ratio()
    
    def find_best_match(self, user_question: str, threshold: float = 0.6) -> Tuple[Optional[str], float]:
        """
        –ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–æ—Ç–≤–µ—Ç, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å) –∏–ª–∏ (None, 0)
        """
        if not self.kb_data:
            return None, 0.0
        
        normalized_question = self.preprocessor.normalize_text(user_question)
        keywords = self.preprocessor.extract_keywords(normalized_question)
        
        best_answer = None
        best_confidence = 0.0
        
        for item in self.kb_data:
            item_question = item.get('question', '')
            item_answer = item.get('answer', '')
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–æ–ø—Ä–æ—Å –∏–∑ –±–∞–∑—ã
            normalized_item = self.preprocessor.normalize_text(item_question)
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            similarity = self._calculate_similarity(normalized_question, normalized_item)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            item_keywords = self.preprocessor.extract_keywords(normalized_item)
            keyword_overlap = len(set(keywords) & set(item_keywords)) / max(len(keywords), 1)
            
            # –ò—Ç–æ–≥–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = (similarity * 0.7) + (keyword_overlap * 0.3)
            
            if confidence > best_confidence:
                best_confidence = confidence
                best_answer = item_answer
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if best_confidence >= threshold:
            return best_answer, best_confidence
        
        return None, 0.0

class DocumentationSearcher:
    """–ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ 1–°"""
    
    def __init__(self):
        self.preprocessor = TextPreprocessor()
    
    def search(self, user_question: str) -> Tuple[str, str]:
        """
        –ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ 1–°
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–∏—Å—Ç–æ—á–Ω–∏–∫, –æ—Ç–≤–µ—Ç)
        """
        normalized = self.preprocessor.normalize_text(user_question)
        keywords = self.preprocessor.extract_keywords(normalized)
        
        # TODO: –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –ø–æ–¥–∫–ª—é—á–∏—Ç—å:
        # 1. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (ChromaDB, Qdrant)
        # 2. RAG —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM (Ollama)
        # 3. API 1–° –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        answers_by_topic = {
            '–Ω–∞–∫–ª–∞–¥–Ω–∞—è': "üì¶ <b>–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –Ω–∞–∫–ª–∞–¥–Ω—ã–º:</b>\n\n–í –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ 1–° —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–∞–∫–ª–∞–¥–Ω–æ–π –æ–ø–∏—Å–∞–Ω–æ –≤ —Ä–∞–∑–¥–µ–ª–µ '–ü—Ä–æ–¥–∞–∂–∏' ‚Üí '–†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–≤–∞—Ä–æ–≤ –∏ —É—Å–ª—É–≥'. –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞, —Å–∫–ª–∞–¥, –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—É –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ.",
            '–æ—Ç—á–µ—Ç': "üìä <b>–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –æ—Ç—á–µ—Ç–∞–º:</b>\n\n–û—Ç—á–µ—Ç—ã –≤ 1–° –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç—á–µ—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –º–µ–Ω—é '–û—Ç—á–µ—Ç—ã'. –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–∏—Å—Ç–µ–º—É –∫–æ–º–ø–æ–Ω–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö (–°–ö–î).",
            '–æ–ø–ª–∞—Ç–∞': "üí∞ <b>–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –ø–ª–∞—Ç–µ–∂–∞–º:</b>\n\n–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –æ–ø–ª–∞—Ç—ã –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ –¥–æ–∫—É–º–µ–Ω—Ç—ã '–ü–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ' –∏–ª–∏ '–ö–∞—Å—Å–æ–≤—ã–π –æ—Ä–¥–µ—Ä'. –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–∞—Ö '–ë–∞–Ω–∫' –∏–ª–∏ '–ö–∞—Å—Å–∞' —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.",
            '–æ—Å—Ç–∞—Ç–æ–∫': "üì¶ <b>–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –æ—Å—Ç–∞—Ç–∫–∞–º:</b>\n\n–û—Å—Ç–∞—Ç–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤ –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —á–µ—Ä–µ–∑ –æ—Ç—á–µ—Ç '–û—Å—Ç–∞—Ç–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å–∫–ª–∞–¥–∞—Ö' –∏–ª–∏ —á–µ—Ä–µ–∑ —Ä–µ–≥–∏—Å—Ç—Ä—ã –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è '–¢–æ–≤–∞—Ä—ã–ù–∞–°–∫–ª–∞–¥–∞—Ö'.",
            '–ø—Ä–∏—Ö–æ–¥–Ω—ã–π': "üì• <b>–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –ø—Ä–∏—Ö–æ–¥–Ω—ã–º –æ—Ä–¥–µ—Ä–∞–º:</b>\n\n–ü—Ä–∏—Ö–æ–¥–Ω—ã–π –æ—Ä–¥–µ—Ä —Å–æ–∑–¥–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ '–°–∫–ª–∞–¥' ‚Üí '–ü—Ä–∏—Ö–æ–¥–Ω—ã–µ –æ—Ä–¥–µ—Ä–∞'. –î–æ–∫—É–º–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–ø—Ä–∏—Ö–æ–¥–æ–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤ –æ—Ç –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤."
        }
        
        # –ò—â–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in keywords:
            for topic, answer in answers_by_topic.items():
                if keyword.startswith(topic) or topic in keyword:
                    return "doc_1c", answer
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        return "doc_1c", self._generate_default_answer(user_question, keywords)
    
    def _generate_default_answer(self, question: str, keywords: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        if keywords:
            keywords_str = ", ".join(keywords[:3])
            return f"üîç <b>–ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ 1–°:</b>\n\n–ü–æ –∑–∞–ø—Ä–æ—Å—É '{question}' —è –Ω–∞—à–µ–ª —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ: {keywords_str}.\n\n–û–¥–Ω–∞–∫–æ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."
        
        return f"üîç <b>–ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ 1–°:</b>\n\n–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∑–∞–ø—Ä–æ—Å—É '{question}' –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ 1–°.\n\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —Ä–∞–∑–¥–µ–ª—É —Å–ø—Ä–∞–≤–∫–∏ –≤ —Å–∞–º–æ–π –ø—Ä–æ–≥—Ä–∞–º–º–µ 1–°."

class IntentClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    
    def __init__(self):
        self.intents = {
            'greeting': ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä—ã–π', 'hello', 'hi', '–Ω–∞—á–∞—Ç—å'],
            'farewell': ['–ø–æ–∫–∞', '–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è', '–≤—ã—Ö–æ–¥', '–∑–∞–∫–æ–Ω—á–∏—Ç—å', '—Å–ø–∞—Å–∏–±–æ'],
            'help': ['–ø–æ–º–æ—â—å', '–ø–æ–º–æ–≥–∏', '—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å', '–∫–æ–º–∞–Ω–¥—ã'],
            'question_1c': ['–∫–∞–∫', '–≥–¥–µ', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–º–æ–∂–Ω–æ –ª–∏', '–∫–∞–∫–æ–π'],
            'document': ['–Ω–∞–∫–ª–∞–¥–Ω–∞—è', '—Å—á–µ—Ç', '–∞–∫—Ç', '–¥–æ–≥–æ–≤–æ—Ä', '–æ—Ä–¥–µ—Ä', '–æ—Ç—á–µ—Ç'],
            'operation': ['—Å–æ–∑–¥–∞—Ç—å', '—É–¥–∞–ª–∏—Ç—å', '–∏–∑–º–µ–Ω–∏—Ç—å', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '–æ—Ç–º–µ–Ω–∏—Ç—å'],
            'search': ['–Ω–∞–π—Ç–∏', '–ø–æ–∏—Å–∫', '–∏—Å–∫–∞—Ç—å', '–≥–¥–µ –Ω–∞–π—Ç–∏', '–∫–∞–∫ –Ω–∞–π—Ç–∏']
        }
    
    def classify(self, text: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–µ"""
        text_lower = text.lower()
        detected_intents = []
        
        for intent, keywords in self.intents.items():
            for keyword in keywords:
                if keyword in text_lower:
                    detected_intents.append(intent)
                    break
        
        return detected_intents if detected_intents else ['unknown']

class NLPEngine:
    """–û—Å–Ω–æ–≤–Ω–æ–π NLP-–¥–≤–∏–∂–æ–∫"""
    
    def __init__(self):
        self.preprocessor = TextPreprocessor()
        self.intent_classifier = IntentClassifier()
        self.kb_searcher = KnowledgeBaseSearcher()
        self.doc_searcher = DocumentationSearcher()
    
    def process_message(self, user_message: str) -> Dict:
        """
        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        normalized = self.preprocessor.normalize_text(user_message)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π
        intents = self.intent_classifier.classify(normalized)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = self.preprocessor.extract_keywords(normalized)
        
        # –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        kb_answer, kb_confidence = self.kb_searcher.find_best_match(user_message)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result = {
            'original_message': user_message,
            'normalized_message': normalized,
            'intents': intents,
            'keywords': keywords,
            'kb_answer': kb_answer,
            'kb_confidence': kb_confidence,
            'has_kb_answer': kb_answer is not None,
            'doc_answer': None,
            'doc_source': None
        }
        
        # –ï—Å–ª–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        if not kb_answer:
            doc_source, doc_answer = self.doc_searcher.search(user_message)
            result['doc_answer'] = doc_answer
            result['doc_source'] = doc_source
        
        return result
    
    def get_final_answer(self, user_message: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        analysis = self.process_message(user_message)
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        if analysis['has_kb_answer']:
            confidence_percent = int(analysis['kb_confidence'] * 100)
            return f"‚úÖ {analysis['kb_answer']}\n\n<i>(–ù–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence_percent}%)</i>"
        
        # –ï—Å–ª–∏ –∏—Å–∫–∞–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        if analysis['doc_answer']:
            source_map = {
                'doc_1c': '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ 1–°',
                'rag': '–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ',
                'api': 'API 1–°'
            }
            source_name = source_map.get(analysis['doc_source'], '–∏—Å—Ç–æ—á–Ω–∏–∫–µ')
            
            return f"{analysis['doc_answer']}\n\n<i>(–ù–∞–π–¥–µ–Ω–æ –≤ {source_name})</i>"
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏
        return "ü§î <b>–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.</b>\n\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n1. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å\n2. –û–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É\n3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–ø—Ä–∞–≤–∫—É –≤ —Å–∞–º–æ–π –ø—Ä–æ–≥—Ä–∞–º–º–µ 1–°"

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä NLP-–¥–≤–∏–∂–∫–∞
nlp_engine = NLPEngine()
