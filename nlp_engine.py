import json
import re
from typing import Tuple, Optional, Dict, List, Any
import difflib

class TextPreprocessor:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    @staticmethod
    def normalize_text(text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        text = text.lower().strip()
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text
    
    @staticmethod
    def extract_keywords(text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        stop_words = {
            '–∫–∞–∫', '–≥–¥–µ', '—á—Ç–æ', '–∫—Ç–æ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º',
            '–º–Ω–µ', '–º–Ω–æ–π', '–º–µ–Ω—è', '—Ç–µ–±–µ', '—Ç–æ–±–æ–π', '—Ç–µ–±—è',
            '—Å–≤–æ–π', '—Å–≤–æ—è', '—Å–≤–æ—ë', '—Å–≤–æ–∏',
            '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏', '—ç—Ç–æ—Ç',
            '–≤–æ—Ç', '—Ç—É—Ç', '—Ç–∞–º', '–∑–¥–µ—Å—å', '—Ç—É–¥–∞',
            '–æ—á–µ–Ω—å', '–ø—Ä–æ—Å—Ç–æ', '–≤–æ–æ–±—â–µ', '—Å–æ–≤—Å–µ–º',
            '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–Ω–∞–¥–æ', '—Ö–æ—á—É', '—Ö–æ—Ç–µ–ª'
        }
        
        words = text.split()
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        return keywords

class FuzzySearcher:
    """–ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    @staticmethod
    def fuzzy_ratio(text1: str, text2: str) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤"""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text1 = text1.lower()
        text2 = text2.lower()
        
        # 1. –ë–∞–∑–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        base_ratio = difflib.SequenceMatcher(None, text1, text2).ratio()
        
        # 2. –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words1 = text1.split()
        words2 = text2.split()
        
        # 3. –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞ (–¥–∞–∂–µ —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)
        common_score = 0
        for w1 in words1:
            for w2 in words2:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if w1 in w2 or w2 in w1:
                    common_score += 1
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞)
                elif len(w1) > 3 and len(w2) > 3:
                    similarity = difflib.SequenceMatcher(None, w1, w2).ratio()
                    if similarity > 0.6:
                        common_score += similarity
        
        word_overlap = common_score / max(len(words1), len(words2), 1)
        
        # 4. –ü–µ—Ä–≤—ã–µ –±—É–∫–≤—ã —Å–ª–æ–≤
        first_letter_score = 0
        if words1 and words2:
            first_match = 0
            for i in range(min(len(words1), len(words2))):
                if words1[i][0] == words2[i][0]:
                    first_match += 1
            first_letter_score = first_match / max(len(words1), len(words2))
        
        # 5. –§–∏–Ω–∞–ª—å–Ω—ã–π score
        fuzzy_score = (base_ratio * 0.4) + (word_overlap * 0.4) + (first_letter_score * 0.2)
        return min(fuzzy_score, 1.0)

class KnowledgeBaseSearcher:
    """–ü–æ–∏—Å–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    def __init__(self, file_path: str = "knowledge_base.json"):
        self.file_path = file_path
        self.kb_data = self._load_knowledge_base()
        self.preprocessor = TextPreprocessor()
        self.fuzzy_searcher = FuzzySearcher()
        self.question_index = self._build_index()
        self.synonym_index = self._build_synonym_index()
    
    def _load_knowledge_base(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(data)} –∑–∞–ø–∏—Å–µ–π")
                return data
        except FileNotFoundError:
            print(f"‚ö†Ô∏è –§–∞–π–ª {self.file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞—é –ø—Ä–∏–º–µ—Ä–Ω—É—é –±–∞–∑—É...")
            return self._create_sample_knowledge_base()
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON: {e}")
            return []
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            return []
    
    def _create_sample_knowledge_base(self) -> List[Dict]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        sample_data = [
            {
                "id": 1,
                "question": "–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –Ω–∞–∫–ª–∞–¥–Ω—É—é?",
                "answer": "‚úÖ üÜï –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –Ω–∞–∫–ª–∞–¥–Ω–æ–π:\n\n–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç:\n1. –ù–∞–∂–º–∏—Ç–µ ¬´–ü—Ä–æ–¥–∞–∂–∏¬ª ‚Üí ¬´–†–µ–∞–ª–∏–∑–∞—Ü–∏—è (–∞–∫—Ç—ã, –Ω–∞–∫–ª–∞–¥–Ω—ã–µ)¬ª\n2. –ö–Ω–æ–ø–∫–∞ ¬´–°–æ–∑–¥–∞—Ç—å¬ª ‚Üí ¬´–¢–æ–≤–∞—Ä—ã (–Ω–∞–∫–ª–∞–¥–Ω–∞—è)¬ª\n3. –ó–∞–ø–æ–ª–Ω–∏—Ç–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è...",
                "tags": ["–Ω–∞–∫–ª–∞–¥–Ω–∞—è", "—Å–æ–∑–¥–∞–Ω–∏–µ", "–¥–æ–∫—É–º–µ–Ω—Ç"],
                "source": "manual",
                "metadata": {"button_text": "üì¶ –ù–æ–≤–∞—è –Ω–∞–∫–ª–∞–¥–Ω–∞—è"}
            },
            {
                "id": 2,
                "question": "–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–¥–∞–∂–∞—Ö?",
                "answer": "–û—Ç—á–µ—Ç –æ –ø—Ä–æ–¥–∞–∂–∞—Ö —Å–æ–∑–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –º–µ–Ω—é ¬´–û—Ç—á–µ—Ç—ã¬ª ‚Üí ¬´–ü—Ä–æ–¥–∞–∂–∏¬ª ‚Üí ¬´–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥–∞–∂¬ª...",
                "tags": ["–æ—Ç—á–µ—Ç", "–ø—Ä–æ–¥–∞–∂–∏", "–∞–Ω–∞–ª–∏—Ç–∏–∫–∞"],
                "source": "manual",
                "metadata": {"button_text": "üìä –û—Ç—á–µ—Ç—ã"}
            }
        ]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        try:
            with open(self.file_path, 'w', encoding='utf-8') as f:
                json.dump(sample_data, f, ensure_ascii=False, indent=2)
        except:
            pass
        
        return sample_data
    
    def _build_index(self) -> Dict[str, List[Dict]]:
        index = {}
        for item in self.kb_data:
            question = item.get('question', '')
            normalized = self.preprocessor.normalize_text(question)
            keywords = self.preprocessor.extract_keywords(normalized)
            for keyword in keywords:
                if keyword not in index:
                    index[keyword] = []
                index[keyword].append(item)
        return index
    
    def _build_synonym_index(self) -> Dict[str, List[str]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤"""
        synonyms = {
            '—Å–æ–∑–¥–∞–Ω–∏–µ': ['—Å–æ–∑–¥–∞—Ç—å', '—Å–æ–∑–¥–∞–π', '—Å–æ–∑–¥–∞–≤–∞—Ç—å', '—Å–æ–∑–¥–∞—é', '—Å–æ–∑–¥–∞–ª'],
            '–Ω–æ–≤–∞—è': ['–Ω–æ–≤—ã–π', '–Ω–æ–≤–æ–µ', '–Ω–æ–≤—ã–µ', '–Ω–æ–≤—É—é', '–Ω–æ–≤–æ–π'],
            '–Ω–∞–∫–ª–∞–¥–Ω–∞—è': ['–Ω–∞–∫–ª–∞–¥–Ω–æ–π', '–Ω–∞–∫–ª–∞–¥–Ω—ã–µ', '–Ω–∞–∫–ª–∞–¥–Ω—ã—Ö', '–Ω–∞–∫–ª–∞–¥–Ω—É—é', '–Ω–∞–∫–ª–∞–¥–Ω—ã–º'],
            '–æ—Ç—á–µ—Ç': ['–æ—Ç—á–µ—Ç–∞', '–æ—Ç—á–µ—Ç—ã', '–æ—Ç—á–µ—Ç–æ–≤', '–æ—Ç—á–µ—Ç–æ–º', '–æ—Ç—á–µ—Ç–∞–º'],
            '–ø–ª–∞—Ç–µ–∂': ['–ø–ª–∞—Ç–µ–∂–∞', '–ø–ª–∞—Ç–µ–∂–∏', '–ø–ª–∞—Ç–µ–∂–µ–π', '–ø–ª–∞—Ç–µ–∂–æ–º', '–ø–ª–∞—Ç–µ–∂–∞–º'],
            '–¥–æ–∫—É–º–µ–Ω—Ç': ['–¥–æ–∫—É–º–µ–Ω—Ç–∞', '–¥–æ–∫—É–º–µ–Ω—Ç—ã', '–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤', '–¥–æ–∫—É–º–µ–Ω—Ç–æ–º', '–¥–æ–∫—É–º–µ–Ω—Ç–∞–º'],
        }
        return synonyms
    
    def _expand_keywords(self, keywords: List[str]) -> List[str]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
        expanded = set(keywords)
        for keyword in keywords:
            if keyword in self.synonym_index:
                expanded.update(self.synonym_index[keyword])
        return list(expanded)
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        return self.fuzzy_searcher.fuzzy_ratio(text1, text2)
    
    def find_best_match(
        self, 
        user_question: str, 
        source_type: Optional[str] = None,
        threshold: float = 0.25
    ) -> Tuple[Optional[Dict], float]:
        if not self.kb_data:
            return None, 0.0
        
        normalized_question = self.preprocessor.normalize_text(user_question)
        keywords = self.preprocessor.extract_keywords(normalized_question)
        expanded_keywords = self._expand_keywords(keywords)
        
        best_item = None
        best_confidence = 0.0
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidate_items = []
        seen_ids = set()
        
        for keyword in expanded_keywords:
            if keyword in self.question_index:
                for item in self.question_index[keyword]:
                    item_id = item.get('id')
                    if item_id not in seen_ids:
                        seen_ids.add(item_id)
                        candidate_items.append(item)
        
        if not candidate_items:
            candidate_items = self.kb_data
        
        for item in candidate_items:
            item_question = item.get('question', '')
            item_source = item.get('source', 'manual')
            
            if source_type and item_source != source_type:
                continue
            
            normalized_item = self.preprocessor.normalize_text(item_question)
            similarity = self._calculate_similarity(normalized_question, normalized_item)
            
            partial_match_score = 0
            for kw in keywords:
                if kw in normalized_item:
                    partial_match_score += 0.2
            
            item_keywords = self.preprocessor.extract_keywords(normalized_item)
            common_keywords = set(keywords) & set(item_keywords)
            keyword_overlap = len(common_keywords) / max(len(keywords), 1)
            
            confidence = (similarity * 0.4) + (keyword_overlap * 0.3) + (partial_match_score * 0.3)
            
            if confidence > best_confidence:
                best_confidence = confidence
                best_item = item
        
        if best_confidence >= threshold:
            return best_item, best_confidence
        
        return None, 0.0
    
    def find_by_exact_question(
        self, 
        question: str, 
        source_type: Optional[str] = None
    ) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –≤–æ–ø—Ä–æ—Å—É"""
        normalized_question = self.preprocessor.normalize_text(question)
        
        for item in self.kb_data:
            item_question = self.preprocessor.normalize_text(item.get('question', ''))
            item_source = item.get('source', 'manual')
            
            if source_type and item_source != source_type:
                continue
            
            if item_question == normalized_question:
                return item
        
        return None

class IntentClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    
    def __init__(self):
        self.intents = {
            'greeting': ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä—ã–π', 'hello', 'hi', '–Ω–∞—á–∞—Ç—å', '–ø—Ä–∏–≤'],
            'farewell': ['–ø–æ–∫–∞', '–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è', '–≤—ã—Ö–æ–¥', '–∑–∞–∫–æ–Ω—á–∏—Ç—å', '—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∫', '–≤—Å–µ–≥–æ'],
            'help': ['–ø–æ–º–æ—â—å', '–ø–æ–º–æ–≥–∏', '—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å', '–∫–æ–º–∞–Ω–¥—ã', '–ø–æ–¥—Å–∫–∞–∂–∏', '–ø–æ—Å–æ–≤–µ—Ç—É–π'],
            'question_1c': ['–∫–∞–∫', '–≥–¥–µ', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–º–æ–∂–Ω–æ –ª–∏', '–∫–∞–∫–æ–π', '—á–µ–º'],
            'document': ['–Ω–∞–∫–ª–∞–¥–Ω–∞—è', '—Å—á–µ—Ç', '–∞–∫—Ç', '–¥–æ–≥–æ–≤–æ—Ä', '–æ—Ä–¥–µ—Ä', '–æ—Ç—á–µ—Ç', '–¥–æ–∫—É–º–µ–Ω—Ç'],
            'operation': ['—Å–æ–∑–¥–∞—Ç—å', '—É–¥–∞–ª–∏—Ç—å', '–∏–∑–º–µ–Ω–∏—Ç—å', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '–æ—Ç–º–µ–Ω–∏—Ç—å', '—Å–¥–µ–ª–∞—Ç—å', '–Ω–∞–ø–∏—Å–∞—Ç—å'],
            'search': ['–Ω–∞–π—Ç–∏', '–ø–æ–∏—Å–∫', '–∏—Å–∫–∞—Ç—å', '–≥–¥–µ –Ω–∞–π—Ç–∏', '–∫–∞–∫ –Ω–∞–π—Ç–∏', '–Ω–∞–π–¥–∏'],
            'button_click': ['button:', 'menu:', '–Ω–∞–∂–∞—Ç—å –∫–Ω–æ–ø–∫—É', '–∫–ª–∏–∫ –ø–æ', '–∫–Ω–æ–ø–∫–∞']
        }
    
    def classify(self, text: str) -> List[str]:
        text_lower = text.lower()
        detected_intents = []
        for intent, keywords in self.intents.items():
            for keyword in keywords:
                if keyword in text_lower:
                    detected_intents.append(intent)
                    break
        return detected_intents if detected_intents else ['unknown']
    
    def is_button_click(self, text: str) -> Tuple[bool, Optional[str], Optional[str]]:
        text_lower = text.lower()
        
        for prefix in ['button:', 'menu:']:
            if text_lower.startswith(prefix):
                parts = text_lower.split(':', 1)
                if len(parts) == 2:
                    return True, prefix.rstrip(':'), parts[1].strip()
        
        button_patterns = [
            (['–Ω–∞–∂–∞—Ç—å –∫–Ω–æ–ø–∫—É', '–Ω–∞–∂–º–∏ –∫–Ω–æ–ø–∫—É', '–Ω–∞–∂—ã –∫–Ω–æ–ø–∫—É', '–Ω–∞–∂–∞—Ç—å–∫–Ω–æ–ø–∫—É'], 'button'),
            (['–∫–ª–∏–∫ –ø–æ –∫–Ω–æ–ø–∫–µ', '–∫–ª–∏–∫–Ω—É—Ç—å –∫–Ω–æ–ø–∫—É', '–∫–ª–∏–∫ –ø–æ', '–∫–ª–∏–∫–Ω—É—Ç—å'], 'button'),
            (['–≤ –º–µ–Ω—é', '–º–µ–Ω—é', '–≤ —Ä–∞–∑–µ–¥–µ–ª', '—Ä–∞–∑–µ–¥–µ–ª'], 'menu'),
            (['—Ä–∞–∑–¥–µ–ª', '—Ä–∞–∑–¥–∏–ª', '—Ä–∞–¥–µ–ª'], 'menu')
        ]
        
        for patterns, source_type in button_patterns:
            for pattern in patterns:
                if pattern in text_lower:
                    start_idx = text_lower.find(pattern) + len(pattern)
                    button_text = text_lower[start_idx:].strip()
                    if button_text:
                        return True, source_type, button_text
        
        return False, None, None

class ButtonHandler:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏–π –∫–Ω–æ–ø–æ–∫ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    def __init__(self, kb_searcher: KnowledgeBaseSearcher):
        self.kb_searcher = kb_searcher
        self.preprocessor = TextPreprocessor()
        # –ú–∞–ø–ø–∏–Ω–≥ –∫–Ω–æ–ø–æ–∫ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ id –∑–∞–ø–∏—Å–µ–π
        self.button_mapping = {
            # –ö–Ω–æ–ø–∫–∏ –∏–∑ –º–µ–Ω—é
            "üì¶ –ù–æ–≤–∞—è –Ω–∞–∫–ª–∞–¥–Ω–∞—è": 1,
            "üîÑ –°–æ–∑–¥–∞—Ç—å –£–ü–î": 3,
            "üöö –¢–¢–ù –¥–ª—è –ø–µ—Ä–µ–≤–æ–∑–∫–∏": 5,
            "üìã –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–∫–ª–∞–¥–Ω—É—é": 7,
            "üí≥ –û–ø–ª–∞—Ç–∞ –ø–æ—Å—Ç–∞–≤—â–∏–∫—É": 31,
            "üí∞ –ü–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞": 27,
            "üíµ –í—ã–¥–∞—á–∞ –ø–æ–¥ –æ—Ç—á–µ—Ç": 28,
            "üßæ –ê–≤–∞–Ω—Å–æ–≤—ã–µ –æ—Ç—á–µ—Ç—ã": 29,
            "üìë –ö–∞—Å—Å–æ–≤–∞—è –∫–Ω–∏–≥–∞": 30,
            "üè¶ –ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ –≤—ã–ø–∏—Å–∫–∏": 17,
            "üìà –ü—Ä–∏–±—ã–ª—å –∏ —É–±—ã—Ç–∫–∏": 9,
            "üí∞ –î–µ–Ω–µ–∂–Ω—ã–π –ø–æ—Ç–æ–∫": 9,
            "üì¶ –û—Å—Ç–∞—Ç–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤": 26,
            "üë• –î–µ–±–∏—Ç–æ—Ä—Å–∫–∞—è –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å": 23,
            "üìä –ü—Ä–æ–¥–∞–∂–∏ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º": 25,
            "üìã –¢–æ–≤–∞—Ä–æ–æ–±–æ—Ä–æ—Ç": 24,
            "üìã –î–æ–∫—É–º–µ–Ω—Ç—ã": 200,
            # –ö–Ω–æ–ø–∫–∏ –∏–∑ –¥—Ä—É–≥–∏—Ö –º–µ–Ω—é
            "üìã –°—á–µ—Ç–∞": 43,
            "üìë –ê–∫—Ç–∞": 12,
            "üìù –î–æ–≥–æ–≤–æ—Ä—ã": 44,
            "üè¢ –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏": 47,
            "‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏": 14,
            "üÜò –ü–æ–º–æ—â—å": None,
        }
    
    def handle_button_click(
        self, 
        source_type: str, 
        button_text: str
    ) -> Optional[Dict]:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –∫–Ω–æ–ø–æ–∫
        if button_text in self.button_mapping:
            item_id = self.button_mapping[button_text]
            if item_id is None:  # –ö–Ω–æ–ø–∫–∞ "–ü–æ–º–æ—â—å"
                return None
            # –ò—â–µ–º –∑–∞–ø–∏—Å—å –ø–æ ID
            for item in self.kb_searcher.kb_data:
                if item.get('id') == item_id:
                    return item
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ –º–∞–ø–ø–∏–Ω–≥–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
        normalized_button = self.preprocessor.normalize_text(button_text)
        
        # –£–¥–∞–ª—è–µ–º —ç–º–æ–¥–∑–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
        normalized_button = re.sub(r'[^\w\s]', ' ', normalized_button)
        normalized_button = re.sub(r'\s+', ' ', normalized_button).strip()
        
        exact_match = self.kb_searcher.find_by_exact_question(
            normalized_button, 
            source_type=source_type
        )
        
        if exact_match:
            return exact_match
        
        # –ü—Ä–æ–±—É–µ–º –ø–æ–∏—Å–∫ –ø–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—é –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = self.preprocessor.extract_keywords(normalized_button)
        if keywords:
            for item in self.kb_searcher.kb_data:
                item_question = self.preprocessor.normalize_text(item.get('question', ''))
                item_tags = [tag.lower() for tag in item.get('tags', [])]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –≤–æ–ø—Ä–æ—Å –∏–ª–∏ —Ç–µ–≥–∏
                for keyword in keywords:
                    if (keyword in item_question or 
                        keyword in item_tags or
                        any(keyword in tag for tag in item_tags)):
                        return item
        
        fuzzy_match, confidence = self.kb_searcher.find_best_match(
            normalized_button,
            source_type=source_type,
            threshold=0.25  # –ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –∫–Ω–æ–ø–æ–∫
        )
        
        if fuzzy_match and confidence >= 0.25:
            return fuzzy_match
        
        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞: –∏—â–µ–º –ø–æ —á–∞—Å—Ç–∏—á–Ω—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º
        any_match, confidence = self.kb_searcher.find_best_match(
            normalized_button,
            threshold=0.15
        )
        
        return any_match

class NLPEngine:
    """–û—Å–Ω–æ–≤–Ω–æ–π NLP-–¥–≤–∏–∂–æ–∫"""
    
    def __init__(self):
        self.preprocessor = TextPreprocessor()
        self.intent_classifier = IntentClassifier()
        self.kb_searcher = KnowledgeBaseSearcher()
        self.button_handler = ButtonHandler(self.kb_searcher)
        self._user_options = {}  # user_id -> {option_number: item}
        print("‚úÖ NLPEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def process_message(self, user_message: str) -> Dict[str, Any]:
        print(f"\nüì® –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: '{user_message}'")
        
        is_button_click, source_type, button_text = self.intent_classifier.is_button_click(user_message)
        
        if is_button_click and source_type and button_text:
            print(f"üéØ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–∞–∫ –Ω–∞–∂–∞—Ç–∏–µ –∫–Ω–æ–ø–∫–∏: {source_type} -> '{button_text}'")
            
            kb_item = self.button_handler.handle_button_click(source_type, button_text)
            
            if kb_item:
                return {
                    'original_message': user_message,
                    'normalized_message': button_text,
                    'intents': ['button_click'],
                    'source_type': source_type,
                    'kb_answer': kb_item.get('answer'),
                    'kb_item': kb_item,
                    'kb_confidence': 1.0,
                    'has_kb_answer': True,
                    'is_button_click': True,
                    'is_fuzzy_match': False
                }
        
        normalized = self.preprocessor.normalize_text(user_message)
        intents = self.intent_classifier.classify(normalized)
        keywords = self.preprocessor.extract_keywords(normalized)
        
        kb_item, kb_confidence = self.kb_searcher.find_best_match(
            user_message, 
            threshold=0.25
        )
        
        is_fuzzy_match = False
        if kb_item and kb_confidence < 0.7:
            original_question = kb_item.get('question', '')
            if original_question.lower() != normalized:
                is_fuzzy_match = True
        
        result = {
            'original_message': user_message,
            'normalized_message': normalized,
            'intents': intents,
            'keywords': keywords,
            'kb_answer': kb_item.get('answer') if kb_item else None,
            'kb_item': kb_item,
            'kb_confidence': kb_confidence,
            'has_kb_answer': kb_item is not None,
            'is_button_click': False,
            'is_fuzzy_match': is_fuzzy_match
        }
        
        return result
    
    def get_final_answer(self, user_id: int, user_message: str) -> str:
        print(f"üîç get_final_answer –≤—ã–∑–≤–∞–Ω –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: '{user_message}'")
        try:
            analysis = self.process_message(user_message)
            
            if analysis['has_kb_answer']:
                kb_item = analysis['kb_item']
                answer = kb_item.get('answer', '')
                confidence = analysis['kb_confidence']
                
                if confidence < 0.4:
                    print(f"üîÑ –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ({confidence:.2f})")
                    clarification_response = self.get_clarification_response(user_id, analysis)
                    return clarification_response
                
                if analysis.get('is_button_click'):
                    source = kb_item.get('source', '')
                    button_text = kb_item.get('metadata', {}).get('button_text', '')
                    
                    if button_text and source in ['menu', 'button']:
                        header = f"üîò **{button_text}**\n\n"
                        return header + answer
                
                confidence_percent = int(confidence * 100)
                
                if analysis.get('is_fuzzy_match'):
                    original_question = kb_item.get('question', '')
                    return f"‚úÖ {answer}\n\n<i>(–í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: '{original_question}'. –ù–∞–π–¥–µ–Ω–æ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence_percent}%)</i>"
                else:
                    return f"‚úÖ {answer}\n\n<i>(–ù–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence_percent}%)</i>"
            
            similar_questions = self._find_similar_questions(user_message)
            if similar_questions:
                return self._create_similar_questions_response(user_id, user_message, similar_questions)
            
            suggestions = self._get_search_suggestions(user_message)
            return f"ü§î <b>–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.</b>\n\n{suggestions}"
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ get_final_answer: {e}")
            import traceback
            traceback.print_exc()
            return f"‚ùå <b>–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞:</b>\n\n{str(e)[:100]}"
    
    def _find_similar_questions(self, user_message: str, limit: int = 5) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ –≤–æ–ø—Ä–æ—Å—ã"""
        similar = []
        normalized_query = self.preprocessor.normalize_text(user_message)
        
        for item in self.kb_searcher.kb_data:
            item_question = self.preprocessor.normalize_text(item.get('question', ''))
            similarity = self.kb_searcher._calculate_similarity(normalized_query, item_question)
            
            if similarity > 0.2:
                similar.append({
                    'item': item,
                    'similarity': similarity,
                    'question': item.get('question', '')
                })
        
        similar.sort(key=lambda x: x['similarity'], reverse=True)
        return similar[:limit]
    
    def _create_similar_questions_response(self, user_id: int, user_query: str, similar_questions: List[Dict]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –æ—Ç–≤–µ—Ç —Å –ø–æ—Ö–æ–∂–∏–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏"""
        if not similar_questions:
            return ""
        
        response = f"üîç <b>–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç.</b>\n\n"
        response += f"<i>–í–æ–∑–º–æ–∂–Ω–æ, –≤–∞–º –ø–æ–¥–æ–π–¥–µ—Ç:</i>\n\n"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        self._user_options[user_id] = {}
        
        for i, sim in enumerate(similar_questions[:3], 1):
            question = sim['question']
            similarity = int(sim['similarity'] * 100)
            response += f"{i}. <b>{question}</b> (—Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity}%)\n"
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
            self._user_options[user_id][i] = sim['item']
        
        response += f"\n<b>–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–º–µ—Ä –≤–∞—Ä–∏–∞–Ω—Ç–∞ (1-{min(3, len(similar_questions))})</b>"
        
        print(f"üìù –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –æ–ø—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {list(self._user_options[user_id].keys())}")
        return response
    
    def get_clarification_response(self, user_id: int, analysis: Dict) -> str:
        kb_item = analysis.get('kb_item')
        if not kb_item:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."
        
        original_q = kb_item.get('question', '')
        item_tags = kb_item.get('tags', [])
        item_id = kb_item.get('id')
        
        category_questions = self._get_questions_by_categories(
            item_tags, 
            exclude_id=item_id,
            min_relevance=0.2
        )
        
        return self._create_interactive_clarification(
            user_id,
            original_q,
            category_questions,
            "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å",
            user_query=analysis.get('original_message', '')
        )
    
    def _get_questions_by_categories(
        self, 
        categories: List[str], 
        exclude_id: Optional[int] = None,
        limit: int = 4,
        min_relevance: float = 0.1
    ) -> List[Dict]:
        if not categories:
            return []
        
        categorized_items = []
        
        for item in self.kb_searcher.kb_data:
            if exclude_id and item.get('id') == exclude_id:
                continue
                
            item_tags = item.get('tags', [])
            common_tags = set(categories) & set(item_tags)
            
            if common_tags:
                relevance_score = len(common_tags) / len(categories)
                
                if relevance_score >= min_relevance:
                    categorized_items.append({
                        'item': item,
                        'relevance': relevance_score,
                        'question': item.get('question', ''),
                        'tags': item_tags
                    })
        
        categorized_items.sort(key=lambda x: x['relevance'], reverse=True)
        return categorized_items[:limit]
    
    def _create_interactive_clarification(
        self, 
        user_id: int,
        original_question: str,
        alternative_questions: List[Dict],
        intent_description: str,
        user_query: str = ""
    ) -> str:
        if not alternative_questions:
            return (
                "ü§î **–ú–Ω–µ –Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω–µ–Ω–∏–µ.**\n\n"
                f"–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É **¬´{user_query[:50]}...¬ª** —è –Ω–∞—à–µ–ª:\n"
                f"**¬´{original_question}¬ª**\n\n"
                "*–ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Ç–æ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ:*\n"
                "‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞\n"
                "‚Ä¢ –û–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Ä–∞–∑–¥–µ–ª–∞–º –º–µ–Ω—é\n"
                "‚Ä¢ –°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ"
            )
        
        alternatives_text = []
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –æ–ø—Ü–∏–∏ –∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ
        self._user_options[user_id] = {}
        
        for i, alt in enumerate(alternative_questions[:3], 1):
            question = alt['question']
            tags_preview = ", ".join(alt.get('tags', [])[:2]) if alt.get('tags') else ""
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—Ü–∏—é
            self._user_options[user_id][i] = alt['item']
            
            if tags_preview:
                alternatives_text.append(f"{i}. üîπ **{question}** *({tags_preview})*")
            else:
                alternatives_text.append(f"{i}. üîπ **{question}**")
        
        message = (
            f"üîç **–ù—É–∂–Ω–æ —É—Ç–æ—á–Ω–µ–Ω–∏–µ**\n\n"
            f"–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É —è –Ω–∞—à–µ–ª –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤:\n\n"
            f"{chr(10).join(alternatives_text)}\n\n"
            f"**–ö–∞–∫–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –≤–∞–º –Ω—É–∂–µ–Ω?**\n"
            f"‚Ä¢ –û—Ç–≤–µ—Ç—å—Ç–µ –Ω–æ–º–µ—Ä–æ–º (1-{len(alternatives_text)}) –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–±–æ—Ä–∞\n"
            f"‚Ä¢ –ò–ª–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –∑–∞–ø—Ä–æ—Å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ\n"
            f"‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–Ω–æ–ø–∫–∏ –º–µ–Ω—é –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞\n\n"
            f"*–¢–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å: ¬´{user_query}¬ª*"
        )
        
        print(f"üìù –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –æ–ø—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {list(self._user_options[user_id].keys())}")
        return message
    
    def _get_search_suggestions(self, query: str) -> str:
        normalized = self.preprocessor.normalize_text(query)
        keywords = self.preprocessor.extract_keywords(normalized)
        
        similar_questions = []
        
        for item in self.kb_searcher.kb_data[:10]:
            item_question = self.preprocessor.normalize_text(item.get('question', ''))
            item_keywords = self.preprocessor.extract_keywords(item_question)
            common = set(keywords) & set(item_keywords)
            
            if len(common) >= 1 and item_question not in similar_questions:
                similar_questions.append(item_question)
            
            if len(similar_questions) >= 3:
                break
        
        suggestions = "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
        suggestions += "1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–Ω–æ–ø–∫–∏ –º–µ–Ω—é\n"
        suggestions += "2. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å\n"
        
        if similar_questions:
            suggestions += "3. –í–æ–∑–º–æ–∂–Ω–æ, –≤–∞–º –Ω—É–∂–µ–Ω –æ–¥–∏–Ω –∏–∑ —ç—Ç–∏—Ö —Ä–∞–∑–¥–µ–ª–æ–≤:\n"
            for i, q in enumerate(similar_questions, 1):
                suggestions += f"   ‚Ä¢ {q}\n"
        
        suggestions += "4. –û–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É"
        
        return suggestions
    
    def get_option_selection(self, user_id: int, option_number: int) -> Optional[str]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ –æ–ø—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"""
        print(f"üîç –í—ã–±–æ—Ä –æ–ø—Ü–∏–∏ {option_number} –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
        print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –æ–ø—Ü–∏–∏: {self._user_options.get(user_id, {})}")
        
        if user_id not in self._user_options:
            print(f"‚ö†Ô∏è –ù–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –æ–ø—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            return None
        
        if option_number not in self._user_options[user_id]:
            print(f"‚ö†Ô∏è –û–ø—Ü–∏—è {option_number} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(self._user_options[user_id].keys())}")
            return None
        
        selected = self._user_options[user_id][option_number]
        answer = selected.get('answer', '–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞')
        
        # –û—á–∏—â–∞–µ–º –æ–ø—Ü–∏–∏ –ø–æ—Å–ª–µ –≤—ã–±–æ—Ä–∞
        self._user_options[user_id] = {}
        
        return answer

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä NLP-–¥–≤–∏–∂–∫–∞
nlp_engine = NLPEngine()
