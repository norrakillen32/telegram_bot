import json
import re
from typing import Tuple, Optional, Dict, List, Any
import difflib
from enum import Enum

class TextPreprocessor:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    @staticmethod
    def normalize_text(text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        text = text.lower().strip()
        text = re.sub(r'[^\w\s]', ' ', text)  # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        text = re.sub(r'\s+', ' ', text)      # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        return text
    
    @staticmethod
    def extract_keywords(text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        stop_words = {
            '–∫–∞–∫', '–≥–¥–µ', '—á—Ç–æ', '–∫—Ç–æ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º',
            '–º–Ω–µ', '–º–Ω–æ–π', '–º–µ–Ω—è', '—Ç–µ–±–µ', '—Ç–æ–±–æ–π', '—Ç–µ–±—è',
            '—Å–≤–æ–π', '—Å–≤–æ—è', '—Å–≤–æ—ë', '—Å–≤–æ–∏',
            '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏', '—ç—Ç–æ—Ç',
            '–≤–æ—Ç', '—Ç—É—Ç', '—Ç–∞–º', '–∑–¥–µ—Å—å', '—Ç—É–¥–∞',
            '–æ—á–µ–Ω—å', '–ø—Ä–æ—Å—Ç–æ', '–≤–æ–æ–±—â–µ', '—Å–æ–≤—Å–µ–º',
            '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–Ω–∞–¥–æ', '—Ö–æ—á—É', '—Ö–æ—Ç–µ–ª'
        }
        
        words = text.split()
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        return keywords
    
    @staticmethod
    def get_word_variations(word: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π —Å–ª–æ–≤–∞ –¥–ª—è —É—á–µ—Ç–∞ –æ–ø–µ—á–∞—Ç–æ–∫"""
        variations = [word]
        
        # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏ –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        common_typos = {
            '–∞': ['–æ'], '–æ': ['–∞'], '–µ': ['—ç'], '–∏': ['–π', '—ã'],
            '—Ç': ['—Ç—Ç', '–¥'], '–ø': ['–ø–ø', '–±'], '–∫': ['–∫–∫', '–≥'],
            '—Å': ['—Å—Å', '–∑'], '–≤': ['–≤–≤', '—Ñ']
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å –∑–∞–º–µ–Ω–æ–π –ø–æ—Ö–æ–∂–∏—Ö –±—É–∫–≤
        for i, char in enumerate(word):
            if char in common_typos:
                for replacement in common_typos[char]:
                    variation = word[:i] + replacement + word[i+1:]
                    variations.append(variation)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏/–ª–∏—à–Ω–∏–º–∏ –±—É–∫–≤–∞–º–∏ (–¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤)
        if len(word) > 3:
            # –ü—Ä–æ–ø—É—Å–∫ –æ–¥–Ω–æ–π –±—É–∫–≤—ã
            for i in range(len(word)):
                variations.append(word[:i] + word[i+1:])
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∏—à–Ω–µ–π –±—É–∫–≤—ã (–ø–æ–≤—Ç–æ—Ä)
            for i in range(len(word)-1):
                if word[i] == word[i+1]:
                    variations.append(word[:i] + word[i+1:])
        
        return list(set(variations))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏

class FuzzySearcher:
    """–ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    @staticmethod
    def fuzzy_ratio(text1: str, text2: str) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
        # –ë–∞–∑–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        base_ratio = difflib.SequenceMatcher(None, text1, text2).ratio()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        words1 = text1.split()
        words2 = text2.split()
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞–º
        word_overlap = len(set(words1) & set(words2)) / max(len(set(words1)), 1)
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –±—É–∫–≤
        first_letter_score = 0
        if words1 and words2:
            if words1[0][0] == words2[0][0]:
                first_letter_score = 0.2
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
        fuzzy_score = (base_ratio * 0.6) + (word_overlap * 0.3) + (first_letter_score * 0.1)
        
        return fuzzy_score
    
    @staticmethod
    def find_best_fuzzy_match(query: str, candidates: List[str], threshold: float = 0.5) -> Tuple[Optional[str], float]:
        """–ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ –Ω–µ—á–µ—Ç–∫–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        if not candidates:
            return None, 0.0
        
        best_match = None
        best_score = 0.0
        
        for candidate in candidates:
            score = FuzzySearcher.fuzzy_ratio(query, candidate)
            if score > best_score:
                best_score = score
                best_match = candidate
        
        if best_score >= threshold:
            return best_match, best_score
        
        return None, 0.0
    
    @staticmethod
    def soundex_rus(word: str) -> str:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π Soundex –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
        if not word:
            return ""
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–π –±—É–∫–≤—ã
        first_char = word[0].upper()
        
        # –ö–æ–¥—ã –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –±—É–∫–≤
        codes = {
            '–∞': '0', '–±': '1', '–≤': '2', '–≥': '3', '–¥': '4', '–µ': '0', '—ë': '0',
            '–∂': '1', '–∑': '2', '–∏': '0', '–π': '0', '–∫': '3', '–ª': '4', '–º': '5',
            '–Ω': '6', '–æ': '0', '–ø': '1', '—Ä': '2', '—Å': '3', '—Ç': '4', '—É': '0',
            '—Ñ': '1', '—Ö': '2', '—Ü': '3', '—á': '4', '—à': '5', '—â': '6', '—ä': '0',
            '—ã': '0', '—å': '0', '—ç': '0', '—é': '0', '—è': '0'
        }
        
        # –ö–æ–¥–∏—Ä—É–µ–º —Å–ª–æ–≤–æ
        encoded = first_char
        
        for char in word[1:].lower():
            code = codes.get(char, '0')
            if code != '0' and (not encoded or encoded[-1] != code):
                encoded += code
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 4 —Å–∏–º–≤–æ–ª–æ–≤
        encoded = (encoded + '000')[:4]
        
        return encoded
    
    @staticmethod
    def soundex_match(query: str, target: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ Soundex"""
        query_soundex = FuzzySearcher.soundex_rus(query)
        target_soundex = FuzzySearcher.soundex_rus(target)
        
        return query_soundex == target_soundex

class KnowledgeBaseSearcher:
    """–ü–æ–∏—Å–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    def __init__(self, file_path: str = "knowledge_base.json"):
        self.file_path = file_path
        self.kb_data = self._load_knowledge_base()
        self.preprocessor = TextPreprocessor()
        self.fuzzy_searcher = FuzzySearcher()
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.question_index = self._build_index()
    
    def _load_knowledge_base(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
                return data
        except (FileNotFoundError, json.JSONDecodeError) as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            return []
    
    def _build_index(self) -> Dict[str, List[Dict]]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        index = {}
        
        for item in self.kb_data:
            question = item.get('question', '')
            normalized = self.preprocessor.normalize_text(question)
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            keywords = self.preprocessor.extract_keywords(normalized)
            for keyword in keywords:
                if keyword not in index:
                    index[keyword] = []
                index[keyword].append(item)
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ Soundex
            soundex = self.fuzzy_searcher.soundex_rus(question)
            if soundex not in index:
                index[soundex] = []
            index[soundex].append(item)
        
        return index
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
        return self.fuzzy_searcher.fuzzy_ratio(text1, text2)
    
    def find_best_match(
        self, 
        user_question: str, 
        source_type: Optional[str] = None,
        threshold: float = 0.4  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —É—á–µ—Ç–∞ –æ–ø–µ—á–∞—Ç–æ–∫
    ) -> Tuple[Optional[Dict], float]:
        """
        –ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫
        """
        if not self.kb_data:
            return None, 0.0
        
        normalized_question = self.preprocessor.normalize_text(user_question)
        keywords = self.preprocessor.extract_keywords(normalized_question)
        
        best_item = None
        best_confidence = 0.0
        
        # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å (–±—ã—Å—Ç—Ä—ã–π)
        candidate_items = set()
        
        for keyword in keywords:
            if keyword in self.question_index:
                candidate_items.update(self.question_index[keyword])
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å, –∏—â–µ–º –≤–æ –≤—Å–µ–π –±–∞–∑–µ
        if not candidate_items:
            candidate_items = self.kb_data
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —É—á–µ—Ç–∞ –æ–ø–µ—á–∞—Ç–æ–∫
        query_variations = []
        for keyword in keywords[:3]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
            variations = self.preprocessor.get_word_variations(keyword)
            query_variations.extend(variations)
        
        for item in candidate_items:
            item_question = item.get('question', '')
            item_source = item.get('source', 'manual')
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if source_type and item_source != source_type:
                continue
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–æ–ø—Ä–æ—Å –∏–∑ –±–∞–∑—ã
            normalized_item = self.preprocessor.normalize_text(item_question)
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —á–µ—Ä–µ–∑ –Ω–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫
            similarity = self._calculate_similarity(normalized_question, normalized_item)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ Soundex
            soundex_match = self.fuzzy_searcher.soundex_match(
                normalized_question[:10],  # –ë–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                normalized_item[:10]
            )
            
            if soundex_match:
                similarity = max(similarity, 0.6)  # –ü–æ–≤—ã—à–∞–µ–º score –ø—Ä–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ –ø–æ Soundex
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
            for variation in query_variations[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≤–µ—Ä–æ–∫
                if variation in normalized_item:
                    similarity = max(similarity, 0.55)  # –ù–µ–±–æ–ª—å—à–æ–π –±–æ–Ω—É—Å
                    break
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            item_keywords = self.preprocessor.extract_keywords(normalized_item)
            common_keywords = set(keywords) & set(item_keywords)
            keyword_overlap = len(common_keywords) / max(len(keywords), 1)
            
            # –ò—Ç–æ–≥–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
            confidence = (similarity * 0.6) + (keyword_overlap * 0.4)
            
            if confidence > best_confidence:
                best_confidence = confidence
                best_item = item
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–Ω–∏–∂–µ –¥–ª—è —É—á–µ—Ç–∞ –æ–ø–µ—á–∞—Ç–æ–∫)
        if best_confidence >= threshold:
            return best_item, best_confidence
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ø–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç—è–º –≤–æ–ø—Ä–æ—Å–∞
        if len(keywords) > 1:
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            for item in self.kb_data:
                if source_type and item.get('source') != source_type:
                    continue
                    
                item_text = self.preprocessor.normalize_text(item.get('question', ''))
                matches = sum(1 for kw in keywords if kw in item_text)
                
                if matches >= 2:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 2 —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    confidence = matches / len(keywords)
                    if confidence > best_confidence:
                        best_confidence = confidence
                        best_item = item
        
        if best_confidence >= threshold * 0.8:  # –ï—â–µ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥
            return best_item, best_confidence
        
        return None, 0.0
    
    def find_by_exact_question(
        self, 
        question: str, 
        source_type: Optional[str] = None
    ) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –≤–æ–ø—Ä–æ—Å—É"""
        normalized_question = self.preprocessor.normalize_text(question)
        
        for item in self.kb_data:
            item_question = self.preprocessor.normalize_text(item.get('question', ''))
            item_source = item.get('source', 'manual')
            
            if source_type and item_source != source_type:
                continue
            
            if item_question == normalized_question:
                return item
        
        return None

class IntentClassifier:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    
    def __init__(self):
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞–º–µ—Ä–µ–Ω–∏–π —Å –≤–µ—Å–∞–º–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        self.intent_patterns = {
            'payment_to_supplier': {
                'primary': ['–æ–ø–ª–∞—Ç[–∏—å]—Ç?—å?', '–ø–ª–∞—Ç–µ–∂', '–ø–µ—Ä–µ—á–∏—Å–ª[–µ–∏]—Ç?—å?', '—Å–ø–∏—Å–∞–Ω[–∏–µ]', '–≤—ã–ø–ª–∞—Ç[–∏]—Ç?—å?'],
                'secondary': ['–ø–æ—Å—Ç–∞–≤—â–∏–∫[—É–∞]?', '–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç—É', '–ø–æ—Å—Ç–∞–≤–∫—É', '–∑–∞ —Ç–æ–≤–∞—Ä[—ã]?', '–∑–∞ —É—Å–ª—É–≥—É'],
                'negative': ['–æ–ø—Ä–∏—Ö–æ–¥', '–ø–æ—Å—Ç—É–ø–ª', '–ø–æ–ª—É—á[–µ–∏]', '–ø—Ä–∏–Ω—è—Ç', '—Ç–æ–≤–∞—Ä[—ã]? –æ—Ç'],
                'weight': 2.0,
                'threshold': 1.5
            },
            'goods_receipt': {
                'primary': ['–æ–ø—Ä–∏—Ö–æ–¥[–∞–æ]–≤[–∞]—Ç?—å?', '–ø–æ—Å—Ç—É–ø–ª[–µ–∏]', '–ø–æ–ª—É—á[–µ–∏]–ª?', '–ø—Ä–∏–Ω—è—Ç[—å]', '–ø—Ä–∏–µ–º–∫[–∞—É]'],
                'secondary': ['—Ç–æ–≤–∞—Ä[—ã]?', '–º–∞—Ç–µ—Ä–∏–∞–ª[—ã]?', '—Ç–º—Ü', '–æ—Ç –ø–æ—Å—Ç–∞–≤—â–∏–∫', '–∫—É–ø–∏–ª', '–∑–∞–∫—É–ø–∏–ª'],
                'negative': ['–æ–ø–ª–∞—Ç', '–ø–ª–∞—Ç–µ–∂', '–ø–µ—Ä–µ—á–∏—Å–ª', '–¥–µ–Ω—å–≥', '—Å–ø–∏—Å–∞–Ω'],
                'weight': 2.0,
                'threshold': 1.5
            },
            'invoice_creation': {
                'primary': ['–Ω–∞–∫–ª–∞–¥–Ω[–∞—É—é]', '—Å—á–µ—Ç[- ]?—Ñ–∞–∫—Ç—É—Ä[—É–∞]?', '—É–ø–¥', '—Ç–æ—Ä–≥[- ]?12'],
                'secondary': ['—Å–æ–∑–¥–∞[—Ç—å]', '–≤—ã–ø–∏—Å–∞—Ç[—å]', '–æ—Ñ–æ—Ä–º–∏—Ç[—å]', '–ø—Ä–æ–≤–µ—Å—Ç–∏'],
                'negative': ['–æ–ø–ª–∞—Ç', '–ø–æ–ª—É—á[–µ–∏]–ª?', '–ø—Ä–∏–Ω—è–ª'],
                'weight': 1.8,
                'threshold': 1.3
            },
            'bank_statement': {
                'primary': ['–≤—ã–ø–∏—Å–∫[–∞—É]', '–±–∞–Ω–∫–æ–≤—Å–∫[–∞—É—é]', '–∑–∞–≥—Ä—É–∑[–∏]—Ç?—å?', '–∏–º–ø–æ—Ä—Ç[–∏]'],
                'secondary': ['–±–∞–Ω–∫[–∞–µ—É]?', '—Å—á–µ—Ç[–∞—É–µ]?', '–æ–ø–µ—Ä–∞—Ü–∏[–∏–π]', '–ø–ª–∞—Ç–µ–∂[–µ–∏]'],
                'negative': ['–∫–∞—Å—Å', '–Ω–∞–ª–∏—á–Ω'],
                'weight': 1.5,
                'threshold': 1.2
            },
            'cash_operations': {
                'primary': ['–∫–∞—Å—Å[–∞—É–µ—ã]', '–Ω–∞–ª–∏—á–Ω[—ã–µ]', '–ø–∫–æ', '—Ä–∫–æ', '–æ—Ä–¥–µ—Ä[–∞—É]'],
                'secondary': ['–ø—Ä–∏—Ö–æ–¥–Ω[–æ–π]', '—Ä–∞—Å—Ö–æ–¥–Ω[–æ–π]', '–≤—ã–¥–∞—Ç[—å]', '–ø–æ–ª—É—á[–∏]—Ç?—å?'],
                'negative': ['–±–µ–∑–Ω–∞–ª', '–±–∞–Ω–∫', '–ø–µ—Ä–µ—á–∏—Å–ª'],
                'weight': 1.5,
                'threshold': 1.2
            },
            'report_generation': {
                'primary': ['–æ—Ç—á–µ—Ç[–∞—É]?', '–≤–µ–¥–æ–º–æ—Å—Ç[—å–∏]', '–∞–Ω–∞–ª–∏–∑[–∞—É]?', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫[–∞—É]'],
                'secondary': ['—Å—Ñ–æ—Ä–º–∏—Ä[–æ–∞]–≤[–∞]—Ç?—å?', '–ø–æ—Å–º–æ—Ç—Ä[–µ]—Ç?—å?', '–ø–æ–ª—É—á–∏—Ç[—å]', '–ø–æ—Å—Ç—Ä–æ–∏—Ç[—å]'],
                'negative': ['–¥–æ–∫—É–º–µ–Ω—Ç', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '—Å–æ–∑–¥–∞—Ç[—å]'],
                'weight': 1.3,
                'threshold': 1.0
            },
            'debt_analysis': {
                'primary': ['–∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç[—å–∏]', '–¥–µ–±–∏—Ç–æ—Ä—Å–∫[–∞—É—é]', '–∫—Ä–µ–¥–∏—Ç–æ—Ä—Å–∫[–∞—É—é]', '–¥–æ–ª–≥[–∏–∞]'],
                'secondary': ['–ø–æ—Å–º–æ—Ç—Ä[–µ]—Ç?—å?', '–ø—Ä–æ–≤–µ—Ä–∏—Ç[—å]', '–ø—Ä–æ–∞–Ω–∞–ª–∏–∑[–∏]', '–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç'],
                'negative': ['–æ–ø–ª–∞—Ç', '–ø–µ—Ä–µ—á–∏—Å–ª', '–ø—Ä–æ–≤–µ—Å—Ç–∏'],
                'weight': 1.4,
                'threshold': 1.1
            },
            'advance_report': {
                'primary': ['–∞–≤–∞–Ω—Å–æ–≤[—ã–π]', '–ø–æ–¥–æ—Ç—á–µ—Ç[–Ω]', '–æ—Ç—á–µ—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏–∫'],
                'secondary': ['—Å–æ–∑–¥–∞[—Ç—å]', '–∑–∞–ø–æ–ª–Ω–∏[—Ç—å]', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '—Å–¥–∞[—Ç—å]'],
                'negative': ['–≤—ã–¥–∞—Ç[—å]', '–ø–æ–ª—É—á[–∏]—Ç?—å?', '–¥–µ–Ω—å–≥[–∏] –ø–æ–¥'],
                'weight': 1.4,
                'threshold': 1.1
            },
            'goods_balance': {
                'primary': ['–æ—Å—Ç–∞—Ç–∫[–∏–∞—É]', '–Ω–∞–ª–∏—á[–∏–µ]', '—Å–∫–ª–∞–¥[–∞–µ—É]', '–∑–∞–ø–∞—Å[—ã–∞]'],
                'secondary': ['—Ç–æ–≤–∞—Ä[—ã]?', '–ø–æ—Å–º–æ—Ç—Ä[–µ]—Ç?—å?', '–ø—Ä–æ–≤–µ—Ä–∏—Ç[—å]', '—Å–∫–æ–ª—å–∫–æ –µ—Å—Ç—å'],
                'negative': ['–ø—Ä–æ–¥–∞–∂', '–æ—Ç–≥—Ä—É–∑–∫', '—Ä–µ–∞–ª–∏–∑–∞—Ü'],
                'weight': 1.3,
                'threshold': 1.0
            },
            'sales_period': {
                'primary': ['–ø—Ä–æ–¥–∞–∂[–∏] –ø–æ', '–¥–∏–Ω–∞–º–∏–∫[–∞—É]', '–ø–µ—Ä–∏–æ–¥[–∞—É—ã]?', '–º–µ—Å—è—Ü[–∞—É—ã]?'],
                'secondary': ['–≥—Ä–∞—Ñ–∏–∫', '—Ç–µ–Ω–¥–µ–Ω—Ü', '—Å—Ä–∞–≤–Ω–µ–Ω[–∏–µ]'],
                'negative': ['–æ–ø—Ä–∏—Ö–æ–¥', '–ø–æ—Å—Ç—É–ø–ª', '–∑–∞–∫—É–ø–∫'],
                'weight': 1.3,
                'threshold': 1.0
            },
            'greeting': {
                'primary': ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä[—ã–π–æ–µ]', 'hello', 'hi', '–∑–¥—Ä–∞—Å—Ç–µ'],
                'secondary': [],
                'negative': [],
                'weight': 3.0,
                'threshold': 0.5
            },
            'farewell': {
                'primary': ['–ø–æ–∫–∞', '–¥–æ —Å–≤–∏–¥–∞–Ω[–∏—è]', '–≤—ã—Ö–æ–¥', '–∑–∞–∫–æ–Ω—á[–∏]—Ç?—å?', '—Å–ø–∞—Å–∏–±–æ'],
                'secondary': [],
                'negative': [],
                'weight': 3.0,
                'threshold': 0.5
            },
            'help_request': {
                'primary': ['–ø–æ–º–æ—â[—å–∏]', '–ø–æ–º–æ–≥[–∏]', '–ø–æ–¥—Å–∫–∞–∂[–∏]', '–ø–æ—Å–æ–≤–µ—Ç—É[–π–∏]'],
                'secondary': ['—á—Ç–æ —Ç—ã —É–º–µ–µ—à[—å]', '–∫–æ–º–∞–Ω–¥[—ã]', '–∏–Ω—Å—Ç—Ä—É–∫—Ü[–∏—è]'],
                'negative': [],
                'weight': 2.5,
                'threshold': 0.8
            },
            'button_click': {
                'primary': ['button:', 'menu:', '–∫–Ω–æ–ø–∫[–∞—É–∏]', '–∫–ª–∏–∫[–∞—É]'],
                'secondary': ['–Ω–∞–∂–∞—Ç[—å]', '–Ω–∞–∂–º[–∏]', '–≤—ã–±—Ä–∞—Ç[—å]', '–Ω–∞–∂—ã', '–∫–ª–∏–∫–Ω[—É—Ç—å]'],
                'negative': [],
                'weight': 2.0,
                'threshold': 0.7
            },
            'unknown': {
                'primary': [],
                'secondary': [],
                'negative': [],
                'weight': 0.0,
                'threshold': 0.0
            }
        }
    
    def classify(self, text: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–µ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()
        detected_intents = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        for intent_type in ['greeting', 'farewell', 'help_request', 'button_click']:
            if intent_type == 'button_click':
                if 'button:' in text_lower or 'menu:' in text_lower:
                    detected_intents.append('button_click')
            else:
                for pattern in self.intent_patterns[intent_type]['primary']:
                    if re.search(pattern, text_lower):
                        detected_intents.append(intent_type)
                        break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –±–∞–∑–æ–≤—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        if not detected_intents:
            main_intent = self._classify_with_context(text_lower)
            detected_intents.append(main_intent)
        
        return detected_intents if detected_intents else ['unknown']
    
    def _classify_with_context(self, text: str) -> str:
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤"""
        scores = {}
        
        for intent_name, patterns in self.intent_patterns.items():
            if intent_name in ['greeting', 'farewell', 'help_request', 'button_click', 'unknown']:
                continue
                
            score = 0.0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in patterns['primary']:
                if re.search(pattern, text):
                    score += patterns['weight'] * 2.0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ç–æ—Ä–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in patterns['secondary']:
                if re.search(pattern, text):
                    score += patterns['weight'] * 1.0
            
            # –®—Ç—Ä–∞—Ñ—É–µ–º –∑–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
            for pattern in patterns['negative']:
                if re.search(pattern, text):
                    score -= patterns['weight'] * 3.0  # –°–∏–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ
            
            scores[intent_name] = max(score, 0.0)
        
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω—Ç–µ–Ω—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å—á–µ—Ç–æ–º
        if scores:
            best_intent = max(scores, key=scores.get)
            if scores[best_intent] >= self.intent_patterns[best_intent]['threshold']:
                return best_intent
        
        return 'unknown'
    
    def classify_with_context(self, text: str) -> Tuple[str, float]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        text_lower = text.lower()
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—ã–µ –∏–Ω—Ç–µ–Ω—Ç—ã
        if 'button:' in text_lower or 'menu:' in text_lower:
            return 'button_click', 1.0
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        scores = {}
        for intent_name, patterns in self.intent_patterns.items():
            if intent_name in ['greeting', 'farewell', 'help_request', 'button_click', 'unknown']:
                continue
                
            score = 0.0
            primary_matches = 0
            secondary_matches = 0
            negative_matches = 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in patterns['primary']:
                if re.search(pattern, text_lower):
                    score += patterns['weight'] * 2.0
                    primary_matches += 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ç–æ—Ä–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in patterns['secondary']:
                if re.search(pattern, text_lower):
                    score += patterns['weight'] * 1.0
                    secondary_matches += 1
            
            # –®—Ç—Ä–∞—Ñ—É–µ–º –∑–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
            for pattern in patterns['negative']:
                if re.search(pattern, text_lower):
                    score -= patterns['weight'] * 3.0
                    negative_matches += 1
            
            # –£—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            total_matches = primary_matches + secondary_matches
            if total_matches > 0:
                match_ratio = primary_matches / total_matches
                score *= (0.5 + match_ratio * 0.5)
            
            scores[intent_name] = max(score, 0.0)
        
        if not scores:
            return 'unknown', 0.0
        
        best_intent = max(scores, key=scores.get)
        best_score = scores[best_intent]
        threshold = self.intent_patterns[best_intent]['threshold']
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = min(best_score / (threshold * 2), 1.0) if threshold > 0 else 0.5
        
        if best_score >= threshold:
            return best_intent, confidence
        
        return 'unknown', confidence
    
    def is_button_click(self, text: str) -> Tuple[bool, Optional[str], Optional[str]]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–∞–∂–∞—Ç–∏–µ–º –∫–Ω–æ–ø–∫–∏"""
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç—ã: "button:–Ω–∞–∫–ª–∞–¥–Ω—ã–µ" –∏–ª–∏ "menu:–æ—Ç—á–µ—Ç—ã"
        for prefix in ['button:', 'menu:']:
            if text_lower.startswith(prefix):
                parts = text_lower.split(':', 1)
                if len(parts) == 2:
                    return True, prefix.rstrip(':'), parts[1].strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (—Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫)
        button_patterns = [
            (['–Ω–∞–∂–∞—Ç—å –∫–Ω–æ–ø–∫—É', '–Ω–∞–∂–º–∏ –∫–Ω–æ–ø–∫—É', '–Ω–∞–∂—ã –∫–Ω–æ–ø–∫—É', '–Ω–∞–∂–∞—Ç—å–∫–Ω–æ–ø–∫—É'], 'button'),
            (['–∫–ª–∏–∫ –ø–æ –∫–Ω–æ–ø–∫–µ', '–∫–ª–∏–∫–Ω—É—Ç—å –∫–Ω–æ–ø–∫—É', '–∫–ª–∏–∫ –ø–æ', '–∫–ª–∏–∫–Ω—É—Ç—å'], 'button'),
            (['–≤ –º–µ–Ω—é', '–º–µ–Ω—é', '–≤ —Ä–∞–∑–µ–¥–µ–ª', '—Ä–∞–∑–µ–¥–µ–ª'], 'menu'),
            (['—Ä–∞–∑–¥–µ–ª', '—Ä–∞–∑–¥–∏–ª', '—Ä–∞–¥–µ–ª'], 'menu')
        ]
        
        for patterns, source_type in button_patterns:
            for pattern in patterns:
                if pattern in text_lower:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
                    start_idx = text_lower.find(pattern) + len(pattern)
                    button_text = text_lower[start_idx:].strip()
                    if button_text:
                        return True, source_type, button_text
        
        return False, None, None
    
    def get_intent_description(self, intent_name: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–Ω—Ç–µ–Ω—Ç–∞"""
        descriptions = {
            'payment_to_supplier': '–û–ø–ª–∞—Ç–∞ –ø–æ—Å—Ç–∞–≤—â–∏–∫—É',
            'goods_receipt': '–û–ø—Ä–∏—Ö–æ–¥–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ –æ—Ç –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞',
            'invoice_creation': '–°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞–∫–ª–∞–¥–Ω–æ–π –∏–ª–∏ —Å—á–µ—Ç–∞-—Ñ–∞–∫—Ç—É—Ä—ã',
            'bank_statement': '–†–∞–±–æ—Ç–∞ —Å –±–∞–Ω–∫–æ–≤—Å–∫–∏–º–∏ –≤—ã–ø–∏—Å–∫–∞–º–∏',
            'cash_operations': '–ö–∞—Å—Å–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏',
            'report_generation': '–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤',
            'debt_analysis': '–ê–Ω–∞–ª–∏–∑ –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç–∏',
            'advance_report': '–ê–≤–∞–Ω—Å–æ–≤—ã–µ –æ—Ç—á–µ—Ç—ã',
            'goods_balance': '–û—Å—Ç–∞—Ç–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤',
            'sales_period': '–ü—Ä–æ–¥–∞–∂–∏ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º',
            'greeting': '–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ',
            'farewell': '–ü—Ä–æ—â–∞–Ω–∏–µ',
            'help_request': '–ó–∞–ø—Ä–æ—Å –ø–æ–º–æ—â–∏',
            'button_click': '–ù–∞–∂–∞—Ç–∏–µ –∫–Ω–æ–ø–∫–∏',
            'unknown': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å'
        }
        return descriptions.get(intent_name, '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏–Ω—Ç–µ–Ω—Ç')

class ButtonHandler:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏–π –∫–Ω–æ–ø–æ–∫ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    def __init__(self, kb_searcher: KnowledgeBaseSearcher):
        self.kb_searcher = kb_searcher
        self.preprocessor = TextPreprocessor()
    
    def handle_button_click(
        self, 
        source_type: str, 
        button_text: str
    ) -> Optional[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
        print(f"üîò –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–Ω–æ–ø–∫–∏: source={source_type}, text='{button_text}'")
        
        normalized_button = self.preprocessor.normalize_text(button_text)
        
        # 1. –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        exact_match = self.kb_searcher.find_by_exact_question(
            normalized_button, 
            source_type=source_type
        )
        
        if exact_match:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª—è –∫–Ω–æ–ø–∫–∏ '{button_text}'")
            return exact_match
        
        # 2. –ò—â–µ–º —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫ (–Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥)
        fuzzy_match, confidence = self.kb_searcher.find_best_match(
            normalized_button,
            source_type=source_type,
            threshold=0.3  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –∫–Ω–æ–ø–æ–∫
        )
        
        if fuzzy_match and confidence >= 0.3:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –Ω–µ—á–µ—Ç–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            return fuzzy_match
        
        # 3. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º source, –∏—â–µ–º –≤ –ª—é–±–æ–º source
        any_match, confidence = self.kb_searcher.find_best_match(
            normalized_button,
            threshold=0.35
        )
        
        if any_match:
            print(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –¥—Ä—É–≥–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            return any_match
        
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è –∫–Ω–æ–ø–∫–∏ '{button_text}'")
        return None

class NLPEngine:
    """–û—Å–Ω–æ–≤–Ω–æ–π NLP-–¥–≤–∏–∂–æ–∫ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.preprocessor = TextPreprocessor()
        self.intent_classifier = IntentClassifier()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.kb_searcher = KnowledgeBaseSearcher()
        self.button_handler = ButtonHandler(self.kb_searcher)
    
    def process_message(self, user_message: str) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
        """
        print(f"\nüì® –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: '{user_message}'")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –Ω–∞–∂–∞—Ç–∏–µ–º –∫–Ω–æ–ø–∫–∏
        is_button_click, source_type, button_text = self.intent_classifier.is_button_click(
            user_message
        )
        
        if is_button_click and source_type and button_text:
            print(f"üéØ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–∞–∫ –Ω–∞–∂–∞—Ç–∏–µ –∫–Ω–æ–ø–∫–∏: {source_type} -> '{button_text}'")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –∫–Ω–æ–ø–∫—É
            kb_item = self.button_handler.handle_button_click(source_type, button_text)
            
            if kb_item:
                return {
                    'original_message': user_message,
                    'normalized_message': button_text,
                    'intents': ['button_click'],
                    'source_type': source_type,
                    'kb_answer': kb_item.get('answer'),
                    'kb_item': kb_item,
                    'kb_confidence': 1.0,
                    'has_kb_answer': True,
                    'is_button_click': True,
                    'is_fuzzy_match': False
                }
        
        # –û–±—ã—á–Ω–∞—è —Ç–µ–∫—Å—Ç–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        normalized = self.preprocessor.normalize_text(user_message)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        main_intent, intent_confidence = self.intent_classifier.classify_with_context(normalized)
        intent_description = self.intent_classifier.get_intent_description(main_intent)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = self.preprocessor.extract_keywords(normalized)
        
        # –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫
        kb_item, kb_confidence = self.kb_searcher.find_best_match(
            user_message, 
            threshold=0.35
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —ç—Ç–æ fuzzy match
        is_fuzzy_match = False
        if kb_item and kb_confidence < 0.7:
            original_question = kb_item.get('question', '')
            if original_question.lower() != normalized:
                is_fuzzy_match = True
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result = {
            'original_message': user_message,
            'normalized_message': normalized,
            'main_intent': main_intent,
            'intent_description': intent_description,
            'intent_confidence': intent_confidence,
            'keywords': keywords,
            'kb_answer': kb_item.get('answer') if kb_item else None,
            'kb_item': kb_item,
            'kb_confidence': kb_confidence,
            'has_kb_answer': kb_item is not None,
            'is_button_click': False,
            'is_fuzzy_match': is_fuzzy_match
        }
        
        return result
    def process_query(self, user_message: str) -> Dict[str, Any]:
            """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
            analysis = self.process_message(user_message)
            
            result = {
                'answer': '',
                'needs_clarification': False,
                'options': [],
                'original_question': '',
                'confidence': analysis.get('kb_confidence', 0)
            }
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
            if analysis['has_kb_answer']:
                confidence = analysis['kb_confidence']
                
                # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è (< 65%), –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º —É—Ç–æ—á–Ω–∏—Ç—å
                    if confidence < 0.65:
                        clarification_response, options = self.get_clarification_response(analysis)
                        result['answer'] = clarification_response
                        result['needs_clarification'] = True
                        result['options'] = options
                else:
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç
                    result['answer'] = self._format_standard_response(analysis)
            else:
                result['answer'] = self._get_search_suggestions(user_message)
            
            return result

    def get_final_answer(self, user_message: str) -> str:
        """–°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        result = self.process_query(user_message)
        return result['answer']

    def get_clarification_response(self, analysis: Dict) -> str:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —É—Ç–æ—á–Ω—è—é—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        kb_item = analysis.get('kb_item')
        
        if not kb_item:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."
        
        original_q = kb_item.get('question', '')
        item_tags = kb_item.get('tags', [])
        item_id = kb_item.get('id')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ç–µ–Ω—Ç –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è –ø–æ–∏—Å–∫–∞
        search_categories = item_tags.copy()
        if analysis.get('main_intent') and analysis['main_intent'] != 'unknown':
            search_categories.append(analysis['main_intent'])
        
        # –ò—â–µ–º –≤–æ–ø—Ä–æ—Å—ã –≤ —Ç–µ—Ö –∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
        category_questions = self._get_questions_by_categories(
            search_categories, 
            exclude_id=item_id,
            min_relevance=0.2  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        message = self._create_interactive_clarification(
            original_q,
            category_questions,
            analysis.get('intent_description', '')
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
        options = {}
        for i, alt in enumerate(category_questions, 1):
            options[i] = alt
        
        return message, options
    def _get_questions_by_categories(
        self, 
        categories: List[str], 
        exclude_id: Optional[int] = None,
        limit: int = 4,
        min_relevance: float = 0.1
    ) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤ —Ç–µ–≥–æ–≤"""
        if not categories:
            return []
        
        categorized_items = []
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ç–µ–≥–æ–≤ (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å)
        tag_weights = {
            '–æ–ø–ª–∞—Ç–∞': 2.0, '–ø–æ—Å—Ç–∞–≤—â–∏–∫': 2.0, '–ø—Ä–∏–µ–º–∫–∞': 2.0, '—Ç–æ–≤–∞—Ä': 1.5,
            '–Ω–∞–∫–ª–∞–¥–Ω–∞—è': 2.0, '–æ—Ç—á–µ—Ç': 1.5, '–ø–ª–∞—Ç–µ–∂': 2.0, '–¥–æ–∫—É–º–µ–Ω—Ç': 1.2
        }
        
        for item in self.kb_searcher.kb_data:
            if exclude_id and item.get('id') == exclude_id:
                continue
                
            item_tags = item.get('tags', [])
            
            if not item_tags:
                continue
                
            # –í—ã—á–∏—Å–ª—è–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            weighted_relevance = 0
            matched_tags = []
            
            for category in categories:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–µ–≥–æ–≤
                if category in item_tags:
                    weight = tag_weights.get(category, 1.0)
                    weighted_relevance += weight
                    matched_tags.append(category)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–¥–ª—è –∏–Ω—Ç–µ–Ω—Ç–æ–≤ –∏ —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö —Ç–µ–≥–æ–≤)
                else:
                    for item_tag in item_tags:
                        if category in item_tag or item_tag in category:
                            weight = tag_weights.get(category, 0.8)
                            weighted_relevance += weight * 0.7  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                            matched_tags.append(f"{category}‚âà{item_tag}")
                            break
            
            if weighted_relevance >= min_relevance:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                normalized_relevance = weighted_relevance / len(categories)
                
                categorized_items.append({
                    'item': item,
                    'relevance': normalized_relevance,
                    'question': item.get('question', ''),
                    'tags': item_tags,
                    'matched_tags': list(set(matched_tags))[:3],  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    'source': item.get('source', 'manual')
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏, –ø–æ—Ç–æ–º –ø–æ —Ç–∏–ø—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        categorized_items.sort(
            key=lambda x: (x['relevance'], 1 if x['source'] == 'manual' else 0.5),
            reverse=True
        )
        
        return categorized_items[:limit]

    def _find_similar_questions(
        self, 
        original_question: str,
        tags: List[str], 
        main_intent: str,
        exclude_id: Optional[int] = None,
        limit: int = 4
    ) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ —Ç–µ–≥–∞–º –∏ –∏–Ω—Ç–µ–Ω—Ç—É"""
        similar_items = []

        for item in self.kb_searcher.kb_data:
            if exclude_id and item.get('id') == exclude_id:
                continue

            item_tags = item.get('tags', [])
            item_intent = self._detect_item_intent(item.get('question', ''))

            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ —Ç–µ–≥–∞–º
            tag_relevance = 0
            if tags and item_tags:
                common_tags = set(tags) & set(item_tags)
                tag_relevance = len(common_tags) / len(tags) if tags else 0

            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∏–Ω—Ç–µ–Ω—Ç—É
            intent_relevance = 1.0 if item_intent == main_intent else 0.0

            # –û–±—â–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤–µ—Å–∞)
            # –ó–¥–µ—Å—å —Ç–µ–≥–∏ –∏ –∏–Ω—Ç–µ–Ω—Ç –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –≤–µ—Å
            total_relevance = (tag_relevance + intent_relevance) / 2

            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –∫–∞–∫–∞—è-—Ç–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            if total_relevance > 0:
                similar_items.append({
                    'item': item,
                    'question': item.get('question', ''),
                    'tags': item_tags,
                    'relevance': total_relevance
                })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        similar_items.sort(key=lambda x: x['relevance'], reverse=True)

        return similar_items[:limit]

    def _detect_item_intent(self, question: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        normalized = self.preprocessor.normalize_text(question)
        intent, _ = self.intent_classifier.classify_with_context(normalized)
        return intent

    def _create_interactive_clarification(
        self, 
        original_question: str,
        alternative_questions: List[Dict],
        intent_description: str,
        user_query: str = ""
    ) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —É—Ç–æ—á–Ω—è—é—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        
        if not alternative_questions:
            # –ë–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç, –µ—Å–ª–∏ –Ω–µ—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤
            return (
                "ü§î **–ú–Ω–µ –Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω–µ–Ω–∏–µ.**\n\n"
                f"–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É **¬´{user_query[:50]}...¬ª** —è –Ω–∞—à–µ–ª:\n"
                f"**¬´{original_question}¬ª**\n\n"
                "*–ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Ç–æ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ:*\n"
                "‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞\n"
                "‚Ä¢ –û–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Ä–∞–∑–¥–µ–ª–∞–º –º–µ–Ω—é\n"
                "‚Ä¢ –°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ"
            )
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –ø–æ —Ç–∏–ø—É (manual/button/menu)
        manual_items = [a for a in alternative_questions if a.get('source') == 'manual']
        button_items = [a for a in alternative_questions if a.get('source') in ['button', 'menu']]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤
        alternatives_text = []
        option_counter = 1
        option_map = {}  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –Ω–æ–º–µ—Ä–æ–≤ –æ–ø—Ü–∏–π
        
        # –°–Ω–∞—á–∞–ª–∞ —Ä—É—á–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–æ–±—ã—á–Ω–æ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–µ)
        if manual_items:
            alternatives_text.append("**üìñ –ü–æ–¥—Ä–æ–±–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:**")
            for alt in manual_items[:2]:  # –ù–µ –±–æ–ª—å—à–µ 2 —Ä—É—á–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
                question = alt['question']
                relevance_percent = int(alt['relevance'] * 100)
                tags_preview = self._format_tags_preview(alt.get('matched_tags', []))
                
                option_map[option_counter] = alt
                if tags_preview:
                    alternatives_text.append(f"{option_counter}. {question} *({tags_preview}, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å {relevance_percent}%)*")
                else:
                    alternatives_text.append(f"{option_counter}. {question} *(—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å {relevance_percent}%)*")
                option_counter += 1
        
        # –ü–æ—Ç–æ–º –∫–Ω–æ–ø–∫–∏/–º–µ–Ω—é (–¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞)
        if button_items:
            alternatives_text.append("\n**üîò –ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø:**")
            for alt in button_items[:2]:  # –ù–µ –±–æ–ª—å—à–µ 2 –∫–Ω–æ–ø–æ–∫
                question = alt['question']
                button_text = alt['item'].get('metadata', {}).get('button_text', 'üìå')
                
                option_map[option_counter] = alt
                alternatives_text.append(f"{option_counter}. {button_text} {question}")
                option_counter += 1
        
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        if intent_description and intent_description != '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å':
            category_info = f"**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {intent_description}\n\n"
        else:
            # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —Ç–µ–≥–∞–º
            if alternative_questions and alternative_questions[0].get('matched_tags'):
                main_tags = alternative_questions[0].get('matched_tags', [])[:3]
                category_info = f"**–¢–µ–≥–∏:** {', '.join(main_tags)}\n\n"
            else:
                category_info = ""
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        message = (
            f"üîç **–£—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞**\n\n"
            f"–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É —è –Ω–∞—à–µ–ª –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤:\n\n"
            f"{chr(10).join(alternatives_text)}\n\n"
            f"**–ö–∞–∫–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –≤–∞–º –Ω—É–∂–µ–Ω?**\n"
            f"‚Ä¢ –û—Ç–≤–µ—Ç—å—Ç–µ –Ω–æ–º–µ—Ä–æ–º (1-{option_counter-1}) –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–±–æ—Ä–∞\n"
            f"‚Ä¢ –ò–ª–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –∑–∞–ø—Ä–æ—Å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ\n"
            f"‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–Ω–æ–ø–∫–∏ –º–µ–Ω—é –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞\n\n"
            f"*–¢–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å: ¬´{user_query}¬ª*"
        )
        
        return message
    def _format_tags_preview(self, tags: List[str]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–≥–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not tags:
            return ""
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
        unique_tags = []
        seen = set()
        for tag in tags:
            # –û—á–∏—â–∞–µ–º —Ç–µ–≥–∏ –æ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            clean_tag = tag.replace('‚âà', '~').split('~')[0]
            if clean_tag not in seen and len(clean_tag) > 2:
                seen.add(clean_tag)
                unique_tags.append(clean_tag)
        
        return ", ".join(unique_tags[:3])
    
    def _handle_option_selection(self, option_number: int, current_options: Dict) -> Optional[str]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ –æ–ø—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (–¥–æ–±–∞–≤–∏—Ç—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ –±–æ—Ç–∞)"""
        if option_number in current_options:
            selected = current_options[option_number]
            item = selected['item']
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–π –æ–ø—Ü–∏–∏
            answer = item.get('answer', '')
            source = item.get('source', '')
            
            if source in ['button', 'menu']:
                button_text = item.get('metadata', {}).get('button_text', '')
                return f"üîò **{button_text}**\n\n{answer}"
            else:
                return answer
        
        return None
    def _format_standard_response(self, analysis: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        kb_item = analysis['kb_item']
        answer = kb_item.get('answer', '')
        confidence_percent = int(analysis['kb_confidence'] * 100)

        # –î–ª—è –∫–Ω–æ–ø–æ–∫ –¥–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ
        if analysis.get('is_button_click'):
            source = kb_item.get('source', '')
            button_text = kb_item.get('metadata', {}).get('button_text', '')
            if button_text and source in ['menu', 'button']:
                header = f"üîò **{button_text}**\n\n"
                return header + answer

        # –î–ª—è fuzzy match –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ—è—Å–Ω–µ–Ω–∏–µ
        if analysis.get('is_fuzzy_match'):
            original_question = kb_item.get('question', '')
            return f"‚úÖ {answer}\n\n<i>(–í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: '{original_question}'. –ù–∞–π–¥–µ–Ω–æ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence_percent}%)</i>"
        else:
            return f"‚úÖ {answer}\n\n<i>(–ù–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence_percent}%)</i>"       
# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä NLP-–¥–≤–∏–∂–∫–∞
nlp_engine = NLPEngine()
